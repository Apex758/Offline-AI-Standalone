[
  {
    "id": "chat_1760514719552",
    "title": "Hey",
    "timestamp": "2025-10-15T07:52:06.014Z",
    "messages": [
      {
        "id": "1760514719548",
        "role": "user",
        "content": "Hey",
        "timestamp": "2025-10-15T07:51:59.548Z"
      },
      {
        "id": "1760514720678",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-15T07:52:00.678Z"
      },
      {
        "id": "1760514725445",
        "role": "user",
        "content": "hey",
        "timestamp": "2025-10-15T07:52:05.445Z"
      },
      {
        "id": "1760514726011",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-15T07:52:06.011Z"
      }
    ]
  },
  {
    "id": "chat_1760514829473",
    "title": "Hey",
    "timestamp": "2025-10-15T07:53:49.473Z",
    "messages": [
      {
        "id": "1760514719548",
        "role": "user",
        "content": "Hey",
        "timestamp": "2025-10-15T07:51:59.548Z"
      },
      {
        "id": "1760514720678",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-15T07:52:00.678Z"
      },
      {
        "id": "1760514725445",
        "role": "user",
        "content": "hey",
        "timestamp": "2025-10-15T07:52:05.445Z"
      },
      {
        "id": "1760514726011",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-15T07:52:06.011Z"
      }
    ]
  },
  {
    "id": "chat_1760514829481",
    "title": "Hey",
    "timestamp": "2025-10-15T07:54:37.153Z",
    "messages": [
      {
        "id": "1760514719548",
        "role": "user",
        "content": "Hey",
        "timestamp": "2025-10-15T07:51:59.548Z"
      },
      {
        "id": "1760514720678",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-15T07:52:00.678Z"
      },
      {
        "id": "1760514725445",
        "role": "user",
        "content": "hey",
        "timestamp": "2025-10-15T07:52:05.445Z"
      },
      {
        "id": "1760514726011",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-15T07:52:06.011Z"
      },
      {
        "id": "1760514833456",
        "role": "user",
        "content": "hi",
        "timestamp": "2025-10-15T07:53:53.456Z"
      },
      {
        "id": "1760514877139",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 41 key-value pairs and 444 tensors from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Gemma 3 4b It\nllama_model_loader: - kv   3:                           general.finetune str              = it\nllama_model_loader: - kv   4:                           general.basename str              = gemma-3\nllama_model_loader: - kv   5:                         general.size_label str              = 4B\nllama_model_loader: - kv   6:                            general.license str              = gemma\nllama_model_loader: - kv   7:                   general.base_model.count u32              = 1\nllama_model_loader: - kv   8:                  general.base_model.0.name str              = Gemma 3 4b Pt\nllama_model_loader: - kv   9:          general.base_model.0.organization str              = Google\nllama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/google/gemma-3...\nllama_model_loader: - kv  11:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\nllama_model_loader: - kv  12:                      gemma3.context_length u32              = 131072\nllama_model_loader: - kv  13:                    gemma3.embedding_length u32              = 2560\nllama_model_loader: - kv  14:                         gemma3.block_count u32              = 34\nllama_model_loader: - kv  15:                 gemma3.feed_forward_length u32              = 10240\nllama_model_loader: - kv  16:                gemma3.attention.head_count u32              = 8\nllama_model_loader: - kv  17:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  18:                gemma3.attention.key_length u32              = 256\nllama_model_loader: - kv  19:              gemma3.attention.value_length u32              = 256\nllama_model_loader: - kv  20:                      gemma3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  21:            gemma3.attention.sliding_window u32              = 1024\nllama_model_loader: - kv  22:             gemma3.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  23:                   gemma3.rope.scaling.type str              = linear\nllama_model_loader: - kv  24:                 gemma3.rope.scaling.factor f32              = 8.000000\nllama_model_loader: - kv  25:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  26:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  27:                      tokenizer.ggml.tokens arr[str,262208]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  28:                      tokenizer.ggml.scores arr[f32,262208]  = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,262208]  = [3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  31:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  32:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  35:               tokenizer.ggml.add_sep_token bool             = false\nllama_model_loader: - kv  36:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  37:                    tokenizer.chat_template str              = {{ bos_token }}\\n{%- if messages[0]['r...\nllama_model_loader: - kv  38:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - kv  39:               general.quantization_version u32              = 2\nllama_model_loader: - kv  40:                          general.file_type u32              = 17\nllama_model_loader: - type  f32:  205 tensors\nllama_model_loader: - type q5_K:  204 tensors\nllama_model_loader: - type q6_K:   35 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q5_K - Medium\nprint_info: file size   = 2.63 GiB (5.82 BPW) \nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: printing all EOG tokens:\nload:   - 1 ('<eos>')\nload:   - 106 ('<end_of_turn>')\nload: special tokens cache size = 6415\nload: token to piece cache size = 1.9446 MB\nprint_info: arch             = gemma3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 34\nprint_info: n_head           = 8\nprint_info: n_head_kv        = 4\nprint_info: n_rot            = 256\nprint_info: n_swa            = 1024\nprint_info: is_swa_any       = 1\nprint_info: n_embd_head_k    = 256\nprint_info: n_embd_head_v    = 256\nprint_info: n_gqa            = 2\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 6.2e-02\nprint_info: n_ff             = 10240\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 0.125\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 3.88 B\nprint_info: general.name     = Gemma 3 4b It\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 262208\nprint_info: n_merges         = 0\nprint_info: BOS token        = 2 '<bos>'\nprint_info: EOS token        = 1 '<eos>'\nprint_info: EOT token        = 106 '<end_of_turn>'\nprint_info: UNK token        = 3 '<unk>'\nprint_info: PAD token        = 0 '<pad>'\nprint_info: LF token         = 248 '<0x0A>'\nprint_info: EOG token        = 1 '<eos>'\nprint_info: EOG token        = 106 '<end_of_turn>'\nprint_info: max token length = 48\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_Mapped model buffer size =  2692.37 MiB\n..................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 4096\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = auto\nllama_context: kv_unified    = false\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 0.125\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     1.00 MiB\nllama_kv_cache_iswa: creating non-SWA KV cache, size = 4096 cells\nllama_kv_cache:        CPU KV buffer size =    80.00 MiB\nllama_kv_cache: size =   80.00 MiB (  4096 cells,   5 layers,  1/1 seqs), K (f16):   40.00 MiB, V (f16):   40.00 MiB\nllama_kv_cache_iswa: creating     SWA KV cache, size = 1536 cells\nllama_kv_cache:        CPU KV buffer size =   174.00 MiB\nllama_kv_cache: size =  174.00 MiB (  1536 cells,  29 layers,  1/1 seqs), K (f16):   87.00 MiB, V (f16):   87.00 MiB\nllama_context: Flash Attention was auto, set to enabled\nllama_context:        CPU compute buffer size =   517.12 MiB\nllama_context: graph nodes  = 1369\nllama_context: graph splits = 1\ncommon_init_from_params: added <eos> logit bias = -inf\ncommon_init_from_params: added <end_of_turn> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 12\nmain: chat template is available, enabling conversation muserode (disable it with -n",
        "timestamp": "2025-10-15T07:54:37.139Z"
      }
    ]
  },
  {
    "id": "chat_1760514890209",
    "title": "Hey",
    "timestamp": "2025-10-15T07:54:50.209Z",
    "messages": [
      {
        "id": "1760514719548",
        "role": "user",
        "content": "Hey",
        "timestamp": "2025-10-15T07:51:59.548Z"
      },
      {
        "id": "1760514720678",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-15T07:52:00.678Z"
      },
      {
        "id": "1760514725445",
        "role": "user",
        "content": "hey",
        "timestamp": "2025-10-15T07:52:05.445Z"
      },
      {
        "id": "1760514726011",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-15T07:52:06.011Z"
      },
      {
        "id": "1760514833456",
        "role": "user",
        "content": "hi",
        "timestamp": "2025-10-15T07:53:53.456Z"
      },
      {
        "id": "1760514877139",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 41 key-value pairs and 444 tensors from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Gemma 3 4b It\nllama_model_loader: - kv   3:                           general.finetune str              = it\nllama_model_loader: - kv   4:                           general.basename str              = gemma-3\nllama_model_loader: - kv   5:                         general.size_label str              = 4B\nllama_model_loader: - kv   6:                            general.license str              = gemma\nllama_model_loader: - kv   7:                   general.base_model.count u32              = 1\nllama_model_loader: - kv   8:                  general.base_model.0.name str              = Gemma 3 4b Pt\nllama_model_loader: - kv   9:          general.base_model.0.organization str              = Google\nllama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/google/gemma-3...\nllama_model_loader: - kv  11:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\nllama_model_loader: - kv  12:                      gemma3.context_length u32              = 131072\nllama_model_loader: - kv  13:                    gemma3.embedding_length u32              = 2560\nllama_model_loader: - kv  14:                         gemma3.block_count u32              = 34\nllama_model_loader: - kv  15:                 gemma3.feed_forward_length u32              = 10240\nllama_model_loader: - kv  16:                gemma3.attention.head_count u32              = 8\nllama_model_loader: - kv  17:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  18:                gemma3.attention.key_length u32              = 256\nllama_model_loader: - kv  19:              gemma3.attention.value_length u32              = 256\nllama_model_loader: - kv  20:                      gemma3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  21:            gemma3.attention.sliding_window u32              = 1024\nllama_model_loader: - kv  22:             gemma3.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  23:                   gemma3.rope.scaling.type str              = linear\nllama_model_loader: - kv  24:                 gemma3.rope.scaling.factor f32              = 8.000000\nllama_model_loader: - kv  25:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  26:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  27:                      tokenizer.ggml.tokens arr[str,262208]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  28:                      tokenizer.ggml.scores arr[f32,262208]  = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,262208]  = [3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  31:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  32:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  35:               tokenizer.ggml.add_sep_token bool             = false\nllama_model_loader: - kv  36:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  37:                    tokenizer.chat_template str              = {{ bos_token }}\\n{%- if messages[0]['r...\nllama_model_loader: - kv  38:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - kv  39:               general.quantization_version u32              = 2\nllama_model_loader: - kv  40:                          general.file_type u32              = 17\nllama_model_loader: - type  f32:  205 tensors\nllama_model_loader: - type q5_K:  204 tensors\nllama_model_loader: - type q6_K:   35 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q5_K - Medium\nprint_info: file size   = 2.63 GiB (5.82 BPW) \nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: printing all EOG tokens:\nload:   - 1 ('<eos>')\nload:   - 106 ('<end_of_turn>')\nload: special tokens cache size = 6415\nload: token to piece cache size = 1.9446 MB\nprint_info: arch             = gemma3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 34\nprint_info: n_head           = 8\nprint_info: n_head_kv        = 4\nprint_info: n_rot            = 256\nprint_info: n_swa            = 1024\nprint_info: is_swa_any       = 1\nprint_info: n_embd_head_k    = 256\nprint_info: n_embd_head_v    = 256\nprint_info: n_gqa            = 2\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 6.2e-02\nprint_info: n_ff             = 10240\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 0.125\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 3.88 B\nprint_info: general.name     = Gemma 3 4b It\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 262208\nprint_info: n_merges         = 0\nprint_info: BOS token        = 2 '<bos>'\nprint_info: EOS token        = 1 '<eos>'\nprint_info: EOT token        = 106 '<end_of_turn>'\nprint_info: UNK token        = 3 '<unk>'\nprint_info: PAD token        = 0 '<pad>'\nprint_info: LF token         = 248 '<0x0A>'\nprint_info: EOG token        = 1 '<eos>'\nprint_info: EOG token        = 106 '<end_of_turn>'\nprint_info: max token length = 48\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_Mapped model buffer size =  2692.37 MiB\n..................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 4096\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = auto\nllama_context: kv_unified    = false\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 0.125\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     1.00 MiB\nllama_kv_cache_iswa: creating non-SWA KV cache, size = 4096 cells\nllama_kv_cache:        CPU KV buffer size =    80.00 MiB\nllama_kv_cache: size =   80.00 MiB (  4096 cells,   5 layers,  1/1 seqs), K (f16):   40.00 MiB, V (f16):   40.00 MiB\nllama_kv_cache_iswa: creating     SWA KV cache, size = 1536 cells\nllama_kv_cache:        CPU KV buffer size =   174.00 MiB\nllama_kv_cache: size =  174.00 MiB (  1536 cells,  29 layers,  1/1 seqs), K (f16):   87.00 MiB, V (f16):   87.00 MiB\nllama_context: Flash Attention was auto, set to enabled\nllama_context:        CPU compute buffer size =   517.12 MiB\nllama_context: graph nodes  = 1369\nllama_context: graph splits = 1\ncommon_init_from_params: added <eos> logit bias = -inf\ncommon_init_from_params: added <end_of_turn> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 12\nmain: chat template is available, enabling conversation muserode (disable it with -n",
        "timestamp": "2025-10-15T07:54:37.139Z"
      }
    ]
  },
  {
    "id": "chat_1760514890204",
    "title": "Hey",
    "timestamp": "2025-10-15T07:54:50.204Z",
    "messages": [
      {
        "id": "1760514719548",
        "role": "user",
        "content": "Hey",
        "timestamp": "2025-10-15T07:51:59.548Z"
      },
      {
        "id": "1760514720678",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-15T07:52:00.678Z"
      },
      {
        "id": "1760514725445",
        "role": "user",
        "content": "hey",
        "timestamp": "2025-10-15T07:52:05.445Z"
      },
      {
        "id": "1760514726011",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-15T07:52:06.011Z"
      },
      {
        "id": "1760514833456",
        "role": "user",
        "content": "hi",
        "timestamp": "2025-10-15T07:53:53.456Z"
      },
      {
        "id": "1760514877139",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 41 key-value pairs and 444 tensors from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Gemma 3 4b It\nllama_model_loader: - kv   3:                           general.finetune str              = it\nllama_model_loader: - kv   4:                           general.basename str              = gemma-3\nllama_model_loader: - kv   5:                         general.size_label str              = 4B\nllama_model_loader: - kv   6:                            general.license str              = gemma\nllama_model_loader: - kv   7:                   general.base_model.count u32              = 1\nllama_model_loader: - kv   8:                  general.base_model.0.name str              = Gemma 3 4b Pt\nllama_model_loader: - kv   9:          general.base_model.0.organization str              = Google\nllama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/google/gemma-3...\nllama_model_loader: - kv  11:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\nllama_model_loader: - kv  12:                      gemma3.context_length u32              = 131072\nllama_model_loader: - kv  13:                    gemma3.embedding_length u32              = 2560\nllama_model_loader: - kv  14:                         gemma3.block_count u32              = 34\nllama_model_loader: - kv  15:                 gemma3.feed_forward_length u32              = 10240\nllama_model_loader: - kv  16:                gemma3.attention.head_count u32              = 8\nllama_model_loader: - kv  17:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  18:                gemma3.attention.key_length u32              = 256\nllama_model_loader: - kv  19:              gemma3.attention.value_length u32              = 256\nllama_model_loader: - kv  20:                      gemma3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  21:            gemma3.attention.sliding_window u32              = 1024\nllama_model_loader: - kv  22:             gemma3.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  23:                   gemma3.rope.scaling.type str              = linear\nllama_model_loader: - kv  24:                 gemma3.rope.scaling.factor f32              = 8.000000\nllama_model_loader: - kv  25:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  26:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  27:                      tokenizer.ggml.tokens arr[str,262208]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  28:                      tokenizer.ggml.scores arr[f32,262208]  = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,262208]  = [3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  31:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  32:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  35:               tokenizer.ggml.add_sep_token bool             = false\nllama_model_loader: - kv  36:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  37:                    tokenizer.chat_template str              = {{ bos_token }}\\n{%- if messages[0]['r...\nllama_model_loader: - kv  38:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - kv  39:               general.quantization_version u32              = 2\nllama_model_loader: - kv  40:                          general.file_type u32              = 17\nllama_model_loader: - type  f32:  205 tensors\nllama_model_loader: - type q5_K:  204 tensors\nllama_model_loader: - type q6_K:   35 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q5_K - Medium\nprint_info: file size   = 2.63 GiB (5.82 BPW) \nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: printing all EOG tokens:\nload:   - 1 ('<eos>')\nload:   - 106 ('<end_of_turn>')\nload: special tokens cache size = 6415\nload: token to piece cache size = 1.9446 MB\nprint_info: arch             = gemma3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 34\nprint_info: n_head           = 8\nprint_info: n_head_kv        = 4\nprint_info: n_rot            = 256\nprint_info: n_swa            = 1024\nprint_info: is_swa_any       = 1\nprint_info: n_embd_head_k    = 256\nprint_info: n_embd_head_v    = 256\nprint_info: n_gqa            = 2\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 6.2e-02\nprint_info: n_ff             = 10240\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 0.125\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 3.88 B\nprint_info: general.name     = Gemma 3 4b It\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 262208\nprint_info: n_merges         = 0\nprint_info: BOS token        = 2 '<bos>'\nprint_info: EOS token        = 1 '<eos>'\nprint_info: EOT token        = 106 '<end_of_turn>'\nprint_info: UNK token        = 3 '<unk>'\nprint_info: PAD token        = 0 '<pad>'\nprint_info: LF token         = 248 '<0x0A>'\nprint_info: EOG token        = 1 '<eos>'\nprint_info: EOG token        = 106 '<end_of_turn>'\nprint_info: max token length = 48\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_Mapped model buffer size =  2692.37 MiB\n..................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 4096\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = auto\nllama_context: kv_unified    = false\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 0.125\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     1.00 MiB\nllama_kv_cache_iswa: creating non-SWA KV cache, size = 4096 cells\nllama_kv_cache:        CPU KV buffer size =    80.00 MiB\nllama_kv_cache: size =   80.00 MiB (  4096 cells,   5 layers,  1/1 seqs), K (f16):   40.00 MiB, V (f16):   40.00 MiB\nllama_kv_cache_iswa: creating     SWA KV cache, size = 1536 cells\nllama_kv_cache:        CPU KV buffer size =   174.00 MiB\nllama_kv_cache: size =  174.00 MiB (  1536 cells,  29 layers,  1/1 seqs), K (f16):   87.00 MiB, V (f16):   87.00 MiB\nllama_context: Flash Attention was auto, set to enabled\nllama_context:        CPU compute buffer size =   517.12 MiB\nllama_context: graph nodes  = 1369\nllama_context: graph splits = 1\ncommon_init_from_params: added <eos> logit bias = -inf\ncommon_init_from_params: added <end_of_turn> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 12\nmain: chat template is available, enabling conversation muserode (disable it with -n",
        "timestamp": "2025-10-15T07:54:37.139Z"
      }
    ]
  },
  {
    "id": "chat_1760514908457",
    "title": "hello",
    "timestamp": "2025-10-15T08:06:22.810Z",
    "messages": [
      {
        "id": "1760514908450",
        "role": "user",
        "content": "hello",
        "timestamp": "2025-10-15T07:55:08.450Z"
      },
      {
        "id": "1760514954513",
        "role": "assistant",
        "content": "Hello there! It\u00e2\u20ac\u2122s great to hear from you. \u00f0\u0178\u02dc\u0160 \n\nWhat\u00e2\u20ac\u2122s on your mind today? Are you looking to learn something new, or do you have a specific question you'd like me to help you with? \n\nI'm here to explain things in a way that makes sense, so don't hesitate to ask anything \u00e2\u20ac\u201c no matter how basic it might seem! \n\nTo help me give you the best explanation, could you tell me:\n\n*   **What topic are you interested in learning about?** (e.g., math, science, history, a specific skill, etc.)\n*   **What\u00e2\u20ac\u2122s your current understanding of the topic?** (e.g., \u00e2\u20ac\u0153I\u00e2\u20ac\u2122ve never heard of this before",
        "timestamp": "2025-10-15T07:55:54.513Z"
      },
      {
        "id": "1760515215990",
        "role": "user",
        "content": "hello",
        "timestamp": "2025-10-15T08:00:15.990Z"
      },
      {
        "id": "1760515262196",
        "role": "assistant",
        "content": "Hello there! It's great to hear from you. \u00f0\u0178\u02dc\u0160 \n\nWhat's on your mind today? Are you looking to learn about something specific, or would you like me to suggest a topic? \n\nI'm here to help in any way I can \u00e2\u20ac\u201c whether it's explaining a tricky concept, working through a problem, or just sparking your curiosity. \n\nTo get us started, perhaps you could tell me:\n\n*   **What subject are you interested in?** (e.g., math, science, history, computer science, literature, etc.)\n*   **Is there a particular topic you'd like to discuss?** (e.g., photosynthesis, the French Revolution, Python programming, Shakespeare, etc.)\n*   **Or, if you\u00e2\u20ac\u2122re not sure, would you like me to suggest something interesting?**",
        "timestamp": "2025-10-15T08:01:02.196Z"
      },
      {
        "id": "1760515564787",
        "role": "user",
        "content": "hey",
        "timestamp": "2025-10-15T08:06:04.787Z"
      },
      {
        "id": "1760515582718",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 41 key-value pairs and 444 tensors from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI_4bit.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Gemma 3 4b It\nllama_model_loader: - kv   3:                           general.finetune str              = it\nllama_model_loader: - kv   4:                           general.basename str              = gemma-3\nllama_model_loader: - kv   5:                         general.size_label str              = 4B\nllama_model_loader: - kv   6:                            general.license str              = gemma\nllama_model_loader: - kv   7:                   general.base_model.count u32              = 1\nllama_model_loader: - kv   8:                  general.base_model.0.name str              = Gemma 3 4b Pt\nllama_model_loader: - kv   9:          general.base_model.0.organization str              = Google\nllama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/google/gemma-3...\nllama_model_loader: - kv  11:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\nllama_model_loader: - kv  12:                      gemma3.context_length u32              = 131072\nllama_model_loader: - kv  13:                    gemma3.embedding_length u32              = 2560\nllama_model_loader: - kv  14:                         gemma3.block_count u32              = 34\nllama_model_loader: - kv  15:                 gemma3.feed_forward_length u32              = 10240\nllama_model_loader: - kv  16:                gemma3.attention.head_count u32              = 8\nllama_model_loader: - kv  17:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  18:                gemma3.attention.key_length u32              = 256\nllama_model_loader: - kv  19:              gemma3.attention.value_length u32              = 256\nllama_model_loader: - kv  20:                      gemma3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  21:            gemma3.attention.sliding_window u32              = 1024\nllama_model_loader: - kv  22:             gemma3.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  23:                   gemma3.rope.scaling.type str              = linear\nllama_model_loader: - kv  24:                 gemma3.rope.scaling.factor f32              = 8.000000\nllama_model_loader: - kv  25:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  26:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  27:                      tokenizer.ggml.tokens arr[str,262208]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  28:                      tokenizer.ggml.scores arr[f32,262208]  = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,262208]  = [3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  31:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  32:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  35:               tokenizer.ggml.add_sep_token bool             = false\nllama_model_loader: - kv  36:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  37:                    tokenizer.chat_template str              = {{ bos_token }}\\n{%- if messages[0]['r...\nllama_model_loader: - kv  38:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - kv  39:               general.quantization_version u32              = 2\nllama_model_loader: - kv  40:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  205 tensors\nllama_model_loader: - type q4_K:  204 tensors\nllama_model_loader: - type q6_K:   35 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 2.31 GiB (5.12 BPW) \nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: printing all EOG tokens:\nload:   - 1 ('<eos>')\nload:   - 106 ('<end_of_turn>')\nload: special tokens cache size = 6415\nload: token to piece cache size = 1.9446 MB\nprint_info: arch             = gemma3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 34\nprint_info: n_head           = 8\nprint_info: n_head_kv        = 4\nprint_info: n_rot            = 256\nprint_info: n_swa            = 1024\nprint_info: is_swa_any       = 1\nprint_info: n_embd_head_k    = 256\nprint_info: n_embd_head_v    = 256\nprint_info: n_gqa            = 2\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 6.2e-02\nprint_info: n_ff             = 10240\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 0.125\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 3.88 B\nprint_info: general.name     = Gemma 3 4b It\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 262208\nprint_info: n_merges         = 0\nprint_info: BOS token        = 2 '<bos>'\nprint_info: EOS token        = 1 '<eos>'\nprint_info: EOT token        = 106 '<end_of_turn>'\nprint_info: UNK token        = 3 '<unk>'\nprint_info: PAD token        = 0 '<pad>'\nprint_info: LF token         = 248 '<0x0A>'\nprint_info: EOG token        = 1 '<eos>'\nprint_info: EOG token        = 106 '<end_of_turn>'\nprint_info: max token length = 48\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_REPACK model buffer size =  1458.28 MiB\nload_tensors:   CPU_Mapped model buffer size =  2368.31 MiB\n...............................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 4096\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = auto\nllama_context: kv_unified    = false\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 0.125\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     1.00 MiB\nllama_kv_cache_iswa: creating non-SWA KV cache, size = 4096 cells\nllama_kv_cache:        CPU KV buffer size =    80.00 MiB\nllama_kv_cache: size =   80.00 MiB (  4096 cells,   5 layers,  1/1 seqs), K (f16):   40.00 MiB, V (f16):   40.00 MiB\nllama_kv_cache_iswa: creating     SWA KV cache, size = 1536 cells\nllama_kv_cache:        CPU KV buffer size =   174.00 MiB\nllama_kv_cache: size =  174.00 MiB (  1536 cells,  29 layers,  1/1 seqs), K (f16):   87.00 MiB, V (f16):   87.00 MiB\nllama_context: Flash Attention was auto, set to enabled\nllama_context:        CPU compute buffer size =   517.12 MiB\nllama_context: graph nodes  = 1369\nllama_context: graph splits = 1\ncommon_init_from_params: added <eos> logit bias = -inf\ncommon_init_from_params: added <end_of_turn> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 12\nmain: chat template is available, enabling conversation mode (disable it with -userno-cnv)\n*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?\nmain: chat template example:\n<start_o",
        "timestamp": "2025-10-15T08:06:22.718Z"
      }
    ]
  },
  {
    "id": "chat_1760515598214",
    "title": "hey",
    "timestamp": "2025-10-15T08:06:38.844Z",
    "messages": [
      {
        "id": "1760515598208",
        "role": "user",
        "content": "hey",
        "timestamp": "2025-10-15T08:06:38.208Z"
      },
      {
        "id": "1760515598842",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI_4bit.ggufx'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI_4bit.ggufx\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI_4bit.ggufx', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-15T08:06:38.842Z"
      }
    ]
  },
  {
    "id": "chat_1760518843437",
    "title": "hey",
    "timestamp": "2025-10-15T09:00:44.208Z",
    "messages": [
      {
        "id": "1760518843426",
        "role": "user",
        "content": "hey",
        "timestamp": "2025-10-15T09:00:43.426Z"
      },
      {
        "id": "1760518844203",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI_4bit.ggufx'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI_4bit.ggufx\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI_4bit.ggufx', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-15T09:00:44.203Z"
      }
    ]
  },
  {
    "id": "chat_1760518877986",
    "title": "hey",
    "timestamp": "2025-10-15T09:05:20.703Z",
    "messages": [
      {
        "id": "1760518877982",
        "role": "user",
        "content": "hey",
        "timestamp": "2025-10-15T09:01:17.982Z"
      },
      {
        "id": "1760518918951",
        "role": "assistant",
        "content": "\n\nHey there! It\u00e2\u20ac\u2122s great to chat with you. \u00f0\u0178\u02dc\u0160 \n\nWhat\u00e2\u20ac\u2122s on your mind today? Are you wanting to learn something new, or maybe you\u00e2\u20ac\u2122re struggling with a concept and need a little help? \n\nI\u00e2\u20ac\u2122m here to help you understand things better. Don\u00e2\u20ac\u2122t hesitate to ask anything \u00e2\u20ac\u201c no question is too silly! \n\nTo get us started, maybe you could tell me:\n\n*   **What are you interested in learning about today?** (e.g., math, science, history, a specific skill, etc.)\n*   **Or, if you\u00e2\u20ac\u2122re already thinking about a topic, what specifically about it are you finding confusing?**\n\n> lElOaFm ab_yp eursfe_rs\na\n\nm\npIlnetre_rprruipntte:d   b y  suasmeprl\ning time =     120.57 ms /   266 runs   (    0.45 ms per",
        "timestamp": "2025-10-15T09:01:58.951Z"
      },
      {
        "id": "1760518941220",
        "role": "user",
        "content": "how are you",
        "timestamp": "2025-10-15T09:02:21.220Z"
      },
      {
        "id": "1760518971976",
        "role": "assistant",
        "content": "I'm doing great, thanks for asking! I'm always happy to help and assist with any questions or topics you'd like to explore. It's wonderful to have you here!\n\nI've been designed to provide clear and concise explanations, breaking down complex concepts into bite-sized pieces that are easy to understand. My goal is to make learning fun and accessible, so feel free to ask me anything, and I'll do my best to make it interesting and engaging!\n\nWhat would you like to learn about today? Do you have a specific topic in mind, or would you like me to suggest some fun and educational ideas?\n\nl>l aEmOaF_ pbeyr fu_ssearm\np\nl\ne\nrI_nptreirnrtu:p t e d  sbaym pulsienrg\n time",
        "timestamp": "2025-10-15T09:02:51.976Z"
      },
      {
        "id": "1760519085264",
        "role": "user",
        "content": "where in the world is colded",
        "timestamp": "2025-10-15T09:04:45.264Z"
      },
      {
        "id": "1760519120664",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 29 key-value pairs and 398 tensors from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI_qwen.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen\nllama_model_loader: - kv   3:                         general.size_label str              = 4.0B\nllama_model_loader: - kv   4:                            general.license str              = apache-2.0\nllama_model_loader: - kv   5:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-4B-...\nllama_model_loader: - kv   6:                               general.tags arr[str,1]       = [\"text-generation\"]\nllama_model_loader: - kv   7:                          qwen3.block_count u32              = 36\nllama_model_loader: - kv   8:                       qwen3.context_length u32              = 262144\nllama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 2560\nllama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 9728\nllama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 32\nllama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 5000000.000000\nllama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = [\"\u00c4\u00a0 \u00c4\u00a0\", \"\u00c4\u00a0\u00c4\u00a0 \u00c4\u00a0\u00c4\u00a0\", \"i n\", \"\u00c4\u00a0 t\",...\nllama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  26:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  27:               general.quantization_version u32              = 2\nllama_model_loader: - kv  28:                          general.file_type u32              = 17\nllama_model_loader: - type  f32:  145 tensors\nllama_model_loader: - type q5_K:  216 tensors\nllama_model_loader: - type q6_K:   37 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q5_K - Medium\nprint_info: file size   = 2.69 GiB (5.73 BPW) \nload: printing all EOG tokens:\nload:   - 151643 ('<|endoftext|>')\nload:   - 151645 ('<|im_end|>')\nload:   - 151662 ('<|fim_pad|>')\nload:   - 151663 ('<|repo_name|>')\nload:   - 151664 ('<|file_sep|>')\nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 262144\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 36\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 9728\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = -1\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 5000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 262144\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 4.02 B\nprint_info: general.name     = Qwen\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 '\u00c4\u0160'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_Mapped model buffer size =  2749.97 MiB\n..........................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 4096\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = auto\nllama_context: kv_unified    = false\nllama_context: freq_base     = 5000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (262144) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     0.58 MiB\nllama_kv_cache:        CPU KV buffer size =   576.00 MiB\nllama_kv_cache: size =  576.00 MiB (  4096 cells,  36 layers,  1/1 seqs), K (f16):  288.00 MiB, V (f16):  288.00 MiB\nllama_context: Flash Attention was auto, set to enabled\nllama_context:        CPU compute buffer size =   301.75 MiB\nllama_context: graph nodes  = 1267\nllama_context: graph splits = 1\ncommon_init_from_params: added <|endoftext|> logit bias = -inf\ncommon_init_from_params: added <|im_end|> logit bias = -inf\ncommon_init_from_params: added <|fim_pad|> logit bias = -inf\ncommon_init_from_params: added <|repo_name|> logit bias = -inf\ncommon_init_from_params: added <|file_sep|> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 12\nmain: chat template is available, enabuserling conversation mode (disable it with -no-cnv)\n*** User-spe",
        "timestamp": "2025-10-15T09:05:20.664Z"
      }
    ]
  },
  {
    "id": "chat_1760519135274",
    "title": "hey",
    "timestamp": "2025-10-15T09:25:45.652Z",
    "messages": [
      {
        "id": "1760519135265",
        "role": "user",
        "content": "hey",
        "timestamp": "2025-10-15T09:05:35.265Z"
      },
      {
        "id": "1760519149383",
        "role": "assistant",
        "content": "Hey! \u00f0\u0178\u02dc\u0160 How can I help you today? Whether you've got a question, need help understanding a concept, or just want to chat \u00e2\u20ac\u201d I'm here for you! What would you like to explore? \u00f0\u0178\u0161\u20ac",
        "timestamp": "2025-10-15T09:05:49.383Z"
      },
      {
        "id": "1760520324167",
        "role": "user",
        "content": "how are you",
        "timestamp": "2025-10-15T09:25:24.167Z"
      },
      {
        "id": "1760520345612",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 29 key-value pairs and 398 tensors from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen\nllama_model_loader: - kv   3:                         general.size_label str              = 4.0B\nllama_model_loader: - kv   4:                            general.license str              = apache-2.0\nllama_model_loader: - kv   5:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-4B-...\nllama_model_loader: - kv   6:                               general.tags arr[str,1]       = [\"text-generation\"]\nllama_model_loader: - kv   7:                          qwen3.block_count u32              = 36\nllama_model_loader: - kv   8:                       qwen3.context_length u32              = 262144\nllama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 2560\nllama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 9728\nllama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 32\nllama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 5000000.000000\nllama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = [\"\u00c4\u00a0 \u00c4\u00a0\", \"\u00c4\u00a0\u00c4\u00a0 \u00c4\u00a0\u00c4\u00a0\", \"i n\", \"\u00c4\u00a0 t\",...\nllama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  26:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  27:               general.quantization_version u32              = 2\nllama_model_loader: - kv  28:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  145 tensors\nllama_model_loader: - type q4_K:  216 tensors\nllama_model_loader: - type q6_K:   37 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 2.32 GiB (4.95 BPW) \nload: printing all EOG tokens:\nload:   - 151643 ('<|endoftext|>')\nload:   - 151645 ('<|im_end|>')\nload:   - 151662 ('<|fim_pad|>')\nload:   - 151663 ('<|repo_name|>')\nload:   - 151664 ('<|file_sep|>')\nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 262144\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 36\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 9728\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = -1\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 5000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 262144\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 4.02 B\nprint_info: general.name     = Qwen\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 '\u00c4\u0160'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_REPACK model buffer size =  1683.28 MiB\nload_tensors:   CPU_Mapped model buffer size =  2362.55 MiB\n........................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 4096\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = auto\nllama_context: kv_unified    = false\nllama_context: freq_base     = 5000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (262144) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     0.58 MiB\nllama_kv_cache:        CPU KV buffer size =   576.00 MiB\nllama_kv_cache: size =  576.00 MiB (  4096 cells,  36 layers,  1/1 seqs), K (f16):  288.00 MiB, V (f16):  288.00 MiB\nllama_context: Flash Attention was auto, set to enabled\nllama_context:        CPU compute buffer size =   301.75 MiB\nllama_context: graph nodes  = 1267\nllama_context: graph splits = 1\ncommon_init_from_params: added <|endoftext|> logit bias = -inf\ncommon_init_from_params: added <|im_end|> logit bias = -inf\ncommon_init_from_params: added <|fim_pad|> logit bias = -inf\ncommon_init_from_params: added <|repo_name|> logit bias = -inf\ncommon_init_from_params: added <|file_sep|> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 12\nuser",
        "timestamp": "2025-10-15T09:25:45.612Z"
      }
    ]
  },
  {
    "id": "chat_1760556022138",
    "title": "hey",
    "timestamp": "2025-10-15T19:20:22.138Z",
    "messages": [
      {
        "id": "1760519135265",
        "role": "user",
        "content": "hey",
        "timestamp": "2025-10-15T09:05:35.265Z"
      },
      {
        "id": "1760519149383",
        "role": "assistant",
        "content": "Hey! \u00f0\u0178\u02dc\u0160 How can I help you today? Whether you've got a question, need help understanding a concept, or just want to chat \u00e2\u20ac\u201d I'm here for you! What would you like to explore? \u00f0\u0178\u0161\u20ac",
        "timestamp": "2025-10-15T09:05:49.383Z"
      },
      {
        "id": "1760520324167",
        "role": "user",
        "content": "how are you",
        "timestamp": "2025-10-15T09:25:24.167Z"
      },
      {
        "id": "1760520345612",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 29 key-value pairs and 398 tensors from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen\nllama_model_loader: - kv   3:                         general.size_label str              = 4.0B\nllama_model_loader: - kv   4:                            general.license str              = apache-2.0\nllama_model_loader: - kv   5:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-4B-...\nllama_model_loader: - kv   6:                               general.tags arr[str,1]       = [\"text-generation\"]\nllama_model_loader: - kv   7:                          qwen3.block_count u32              = 36\nllama_model_loader: - kv   8:                       qwen3.context_length u32              = 262144\nllama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 2560\nllama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 9728\nllama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 32\nllama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 5000000.000000\nllama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = [\"\u00c4\u00a0 \u00c4\u00a0\", \"\u00c4\u00a0\u00c4\u00a0 \u00c4\u00a0\u00c4\u00a0\", \"i n\", \"\u00c4\u00a0 t\",...\nllama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  26:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  27:               general.quantization_version u32              = 2\nllama_model_loader: - kv  28:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  145 tensors\nllama_model_loader: - type q4_K:  216 tensors\nllama_model_loader: - type q6_K:   37 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 2.32 GiB (4.95 BPW) \nload: printing all EOG tokens:\nload:   - 151643 ('<|endoftext|>')\nload:   - 151645 ('<|im_end|>')\nload:   - 151662 ('<|fim_pad|>')\nload:   - 151663 ('<|repo_name|>')\nload:   - 151664 ('<|file_sep|>')\nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 262144\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 36\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 9728\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = -1\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 5000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 262144\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 4.02 B\nprint_info: general.name     = Qwen\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 '\u00c4\u0160'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_REPACK model buffer size =  1683.28 MiB\nload_tensors:   CPU_Mapped model buffer size =  2362.55 MiB\n........................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 4096\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = auto\nllama_context: kv_unified    = false\nllama_context: freq_base     = 5000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (262144) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     0.58 MiB\nllama_kv_cache:        CPU KV buffer size =   576.00 MiB\nllama_kv_cache: size =  576.00 MiB (  4096 cells,  36 layers,  1/1 seqs), K (f16):  288.00 MiB, V (f16):  288.00 MiB\nllama_context: Flash Attention was auto, set to enabled\nllama_context:        CPU compute buffer size =   301.75 MiB\nllama_context: graph nodes  = 1267\nllama_context: graph splits = 1\ncommon_init_from_params: added <|endoftext|> logit bias = -inf\ncommon_init_from_params: added <|im_end|> logit bias = -inf\ncommon_init_from_params: added <|fim_pad|> logit bias = -inf\ncommon_init_from_params: added <|repo_name|> logit bias = -inf\ncommon_init_from_params: added <|file_sep|> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 12\nuser",
        "timestamp": "2025-10-15T09:25:45.612Z"
      }
    ]
  },
  {
    "id": "chat_1760556022157",
    "title": "hey",
    "timestamp": "2025-10-15T19:20:22.157Z",
    "messages": [
      {
        "id": "1760519135265",
        "role": "user",
        "content": "hey",
        "timestamp": "2025-10-15T09:05:35.265Z"
      },
      {
        "id": "1760519149383",
        "role": "assistant",
        "content": "Hey! \u00f0\u0178\u02dc\u0160 How can I help you today? Whether you've got a question, need help understanding a concept, or just want to chat \u00e2\u20ac\u201d I'm here for you! What would you like to explore? \u00f0\u0178\u0161\u20ac",
        "timestamp": "2025-10-15T09:05:49.383Z"
      },
      {
        "id": "1760520324167",
        "role": "user",
        "content": "how are you",
        "timestamp": "2025-10-15T09:25:24.167Z"
      },
      {
        "id": "1760520345612",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 29 key-value pairs and 398 tensors from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen\nllama_model_loader: - kv   3:                         general.size_label str              = 4.0B\nllama_model_loader: - kv   4:                            general.license str              = apache-2.0\nllama_model_loader: - kv   5:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-4B-...\nllama_model_loader: - kv   6:                               general.tags arr[str,1]       = [\"text-generation\"]\nllama_model_loader: - kv   7:                          qwen3.block_count u32              = 36\nllama_model_loader: - kv   8:                       qwen3.context_length u32              = 262144\nllama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 2560\nllama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 9728\nllama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 32\nllama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 5000000.000000\nllama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = [\"\u00c4\u00a0 \u00c4\u00a0\", \"\u00c4\u00a0\u00c4\u00a0 \u00c4\u00a0\u00c4\u00a0\", \"i n\", \"\u00c4\u00a0 t\",...\nllama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  26:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  27:               general.quantization_version u32              = 2\nllama_model_loader: - kv  28:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  145 tensors\nllama_model_loader: - type q4_K:  216 tensors\nllama_model_loader: - type q6_K:   37 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 2.32 GiB (4.95 BPW) \nload: printing all EOG tokens:\nload:   - 151643 ('<|endoftext|>')\nload:   - 151645 ('<|im_end|>')\nload:   - 151662 ('<|fim_pad|>')\nload:   - 151663 ('<|repo_name|>')\nload:   - 151664 ('<|file_sep|>')\nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 262144\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 36\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 9728\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = -1\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 5000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 262144\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 4.02 B\nprint_info: general.name     = Qwen\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 '\u00c4\u0160'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_REPACK model buffer size =  1683.28 MiB\nload_tensors:   CPU_Mapped model buffer size =  2362.55 MiB\n........................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 4096\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = auto\nllama_context: kv_unified    = false\nllama_context: freq_base     = 5000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (262144) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     0.58 MiB\nllama_kv_cache:        CPU KV buffer size =   576.00 MiB\nllama_kv_cache: size =  576.00 MiB (  4096 cells,  36 layers,  1/1 seqs), K (f16):  288.00 MiB, V (f16):  288.00 MiB\nllama_context: Flash Attention was auto, set to enabled\nllama_context:        CPU compute buffer size =   301.75 MiB\nllama_context: graph nodes  = 1267\nllama_context: graph splits = 1\ncommon_init_from_params: added <|endoftext|> logit bias = -inf\ncommon_init_from_params: added <|im_end|> logit bias = -inf\ncommon_init_from_params: added <|fim_pad|> logit bias = -inf\ncommon_init_from_params: added <|repo_name|> logit bias = -inf\ncommon_init_from_params: added <|file_sep|> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 12\nuser",
        "timestamp": "2025-10-15T09:25:45.612Z"
      }
    ]
  }
]