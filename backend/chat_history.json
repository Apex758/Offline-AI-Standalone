[
  {
    "id": "chat_1760514719552",
    "title": "Hey",
    "timestamp": "2025-10-15T07:52:06.014Z",
    "messages": [
      {
        "id": "1760514719548",
        "role": "user",
        "content": "Hey",
        "timestamp": "2025-10-15T07:51:59.548Z"
      },
      {
        "id": "1760514720678",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-15T07:52:00.678Z"
      },
      {
        "id": "1760514725445",
        "role": "user",
        "content": "hey",
        "timestamp": "2025-10-15T07:52:05.445Z"
      },
      {
        "id": "1760514726011",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-15T07:52:06.011Z"
      }
    ]
  },
  {
    "id": "chat_1760514829473",
    "title": "Hey",
    "timestamp": "2025-10-15T07:53:49.473Z",
    "messages": [
      {
        "id": "1760514719548",
        "role": "user",
        "content": "Hey",
        "timestamp": "2025-10-15T07:51:59.548Z"
      },
      {
        "id": "1760514720678",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-15T07:52:00.678Z"
      },
      {
        "id": "1760514725445",
        "role": "user",
        "content": "hey",
        "timestamp": "2025-10-15T07:52:05.445Z"
      },
      {
        "id": "1760514726011",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-15T07:52:06.011Z"
      }
    ]
  },
  {
    "id": "chat_1760514829481",
    "title": "Hey",
    "timestamp": "2025-10-15T07:54:37.153Z",
    "messages": [
      {
        "id": "1760514719548",
        "role": "user",
        "content": "Hey",
        "timestamp": "2025-10-15T07:51:59.548Z"
      },
      {
        "id": "1760514720678",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-15T07:52:00.678Z"
      },
      {
        "id": "1760514725445",
        "role": "user",
        "content": "hey",
        "timestamp": "2025-10-15T07:52:05.445Z"
      },
      {
        "id": "1760514726011",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-15T07:52:06.011Z"
      },
      {
        "id": "1760514833456",
        "role": "user",
        "content": "hi",
        "timestamp": "2025-10-15T07:53:53.456Z"
      },
      {
        "id": "1760514877139",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 41 key-value pairs and 444 tensors from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Gemma 3 4b It\nllama_model_loader: - kv   3:                           general.finetune str              = it\nllama_model_loader: - kv   4:                           general.basename str              = gemma-3\nllama_model_loader: - kv   5:                         general.size_label str              = 4B\nllama_model_loader: - kv   6:                            general.license str              = gemma\nllama_model_loader: - kv   7:                   general.base_model.count u32              = 1\nllama_model_loader: - kv   8:                  general.base_model.0.name str              = Gemma 3 4b Pt\nllama_model_loader: - kv   9:          general.base_model.0.organization str              = Google\nllama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/google/gemma-3...\nllama_model_loader: - kv  11:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\nllama_model_loader: - kv  12:                      gemma3.context_length u32              = 131072\nllama_model_loader: - kv  13:                    gemma3.embedding_length u32              = 2560\nllama_model_loader: - kv  14:                         gemma3.block_count u32              = 34\nllama_model_loader: - kv  15:                 gemma3.feed_forward_length u32              = 10240\nllama_model_loader: - kv  16:                gemma3.attention.head_count u32              = 8\nllama_model_loader: - kv  17:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  18:                gemma3.attention.key_length u32              = 256\nllama_model_loader: - kv  19:              gemma3.attention.value_length u32              = 256\nllama_model_loader: - kv  20:                      gemma3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  21:            gemma3.attention.sliding_window u32              = 1024\nllama_model_loader: - kv  22:             gemma3.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  23:                   gemma3.rope.scaling.type str              = linear\nllama_model_loader: - kv  24:                 gemma3.rope.scaling.factor f32              = 8.000000\nllama_model_loader: - kv  25:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  26:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  27:                      tokenizer.ggml.tokens arr[str,262208]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  28:                      tokenizer.ggml.scores arr[f32,262208]  = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,262208]  = [3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  31:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  32:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  35:               tokenizer.ggml.add_sep_token bool             = false\nllama_model_loader: - kv  36:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  37:                    tokenizer.chat_template str              = {{ bos_token }}\\n{%- if messages[0]['r...\nllama_model_loader: - kv  38:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - kv  39:               general.quantization_version u32              = 2\nllama_model_loader: - kv  40:                          general.file_type u32              = 17\nllama_model_loader: - type  f32:  205 tensors\nllama_model_loader: - type q5_K:  204 tensors\nllama_model_loader: - type q6_K:   35 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q5_K - Medium\nprint_info: file size   = 2.63 GiB (5.82 BPW) \nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: printing all EOG tokens:\nload:   - 1 ('<eos>')\nload:   - 106 ('<end_of_turn>')\nload: special tokens cache size = 6415\nload: token to piece cache size = 1.9446 MB\nprint_info: arch             = gemma3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 34\nprint_info: n_head           = 8\nprint_info: n_head_kv        = 4\nprint_info: n_rot            = 256\nprint_info: n_swa            = 1024\nprint_info: is_swa_any       = 1\nprint_info: n_embd_head_k    = 256\nprint_info: n_embd_head_v    = 256\nprint_info: n_gqa            = 2\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 6.2e-02\nprint_info: n_ff             = 10240\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 0.125\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 3.88 B\nprint_info: general.name     = Gemma 3 4b It\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 262208\nprint_info: n_merges         = 0\nprint_info: BOS token        = 2 '<bos>'\nprint_info: EOS token        = 1 '<eos>'\nprint_info: EOT token        = 106 '<end_of_turn>'\nprint_info: UNK token        = 3 '<unk>'\nprint_info: PAD token        = 0 '<pad>'\nprint_info: LF token         = 248 '<0x0A>'\nprint_info: EOG token        = 1 '<eos>'\nprint_info: EOG token        = 106 '<end_of_turn>'\nprint_info: max token length = 48\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_Mapped model buffer size =  2692.37 MiB\n..................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 4096\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = auto\nllama_context: kv_unified    = false\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 0.125\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     1.00 MiB\nllama_kv_cache_iswa: creating non-SWA KV cache, size = 4096 cells\nllama_kv_cache:        CPU KV buffer size =    80.00 MiB\nllama_kv_cache: size =   80.00 MiB (  4096 cells,   5 layers,  1/1 seqs), K (f16):   40.00 MiB, V (f16):   40.00 MiB\nllama_kv_cache_iswa: creating     SWA KV cache, size = 1536 cells\nllama_kv_cache:        CPU KV buffer size =   174.00 MiB\nllama_kv_cache: size =  174.00 MiB (  1536 cells,  29 layers,  1/1 seqs), K (f16):   87.00 MiB, V (f16):   87.00 MiB\nllama_context: Flash Attention was auto, set to enabled\nllama_context:        CPU compute buffer size =   517.12 MiB\nllama_context: graph nodes  = 1369\nllama_context: graph splits = 1\ncommon_init_from_params: added <eos> logit bias = -inf\ncommon_init_from_params: added <end_of_turn> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 12\nmain: chat template is available, enabling conversation muserode (disable it with -n",
        "timestamp": "2025-10-15T07:54:37.139Z"
      }
    ]
  },
  {
    "id": "chat_1760514890209",
    "title": "Hey",
    "timestamp": "2025-10-15T07:54:50.209Z",
    "messages": [
      {
        "id": "1760514719548",
        "role": "user",
        "content": "Hey",
        "timestamp": "2025-10-15T07:51:59.548Z"
      },
      {
        "id": "1760514720678",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-15T07:52:00.678Z"
      },
      {
        "id": "1760514725445",
        "role": "user",
        "content": "hey",
        "timestamp": "2025-10-15T07:52:05.445Z"
      },
      {
        "id": "1760514726011",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-15T07:52:06.011Z"
      },
      {
        "id": "1760514833456",
        "role": "user",
        "content": "hi",
        "timestamp": "2025-10-15T07:53:53.456Z"
      },
      {
        "id": "1760514877139",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 41 key-value pairs and 444 tensors from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Gemma 3 4b It\nllama_model_loader: - kv   3:                           general.finetune str              = it\nllama_model_loader: - kv   4:                           general.basename str              = gemma-3\nllama_model_loader: - kv   5:                         general.size_label str              = 4B\nllama_model_loader: - kv   6:                            general.license str              = gemma\nllama_model_loader: - kv   7:                   general.base_model.count u32              = 1\nllama_model_loader: - kv   8:                  general.base_model.0.name str              = Gemma 3 4b Pt\nllama_model_loader: - kv   9:          general.base_model.0.organization str              = Google\nllama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/google/gemma-3...\nllama_model_loader: - kv  11:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\nllama_model_loader: - kv  12:                      gemma3.context_length u32              = 131072\nllama_model_loader: - kv  13:                    gemma3.embedding_length u32              = 2560\nllama_model_loader: - kv  14:                         gemma3.block_count u32              = 34\nllama_model_loader: - kv  15:                 gemma3.feed_forward_length u32              = 10240\nllama_model_loader: - kv  16:                gemma3.attention.head_count u32              = 8\nllama_model_loader: - kv  17:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  18:                gemma3.attention.key_length u32              = 256\nllama_model_loader: - kv  19:              gemma3.attention.value_length u32              = 256\nllama_model_loader: - kv  20:                      gemma3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  21:            gemma3.attention.sliding_window u32              = 1024\nllama_model_loader: - kv  22:             gemma3.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  23:                   gemma3.rope.scaling.type str              = linear\nllama_model_loader: - kv  24:                 gemma3.rope.scaling.factor f32              = 8.000000\nllama_model_loader: - kv  25:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  26:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  27:                      tokenizer.ggml.tokens arr[str,262208]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  28:                      tokenizer.ggml.scores arr[f32,262208]  = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,262208]  = [3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  31:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  32:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  35:               tokenizer.ggml.add_sep_token bool             = false\nllama_model_loader: - kv  36:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  37:                    tokenizer.chat_template str              = {{ bos_token }}\\n{%- if messages[0]['r...\nllama_model_loader: - kv  38:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - kv  39:               general.quantization_version u32              = 2\nllama_model_loader: - kv  40:                          general.file_type u32              = 17\nllama_model_loader: - type  f32:  205 tensors\nllama_model_loader: - type q5_K:  204 tensors\nllama_model_loader: - type q6_K:   35 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q5_K - Medium\nprint_info: file size   = 2.63 GiB (5.82 BPW) \nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: printing all EOG tokens:\nload:   - 1 ('<eos>')\nload:   - 106 ('<end_of_turn>')\nload: special tokens cache size = 6415\nload: token to piece cache size = 1.9446 MB\nprint_info: arch             = gemma3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 34\nprint_info: n_head           = 8\nprint_info: n_head_kv        = 4\nprint_info: n_rot            = 256\nprint_info: n_swa            = 1024\nprint_info: is_swa_any       = 1\nprint_info: n_embd_head_k    = 256\nprint_info: n_embd_head_v    = 256\nprint_info: n_gqa            = 2\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 6.2e-02\nprint_info: n_ff             = 10240\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 0.125\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 3.88 B\nprint_info: general.name     = Gemma 3 4b It\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 262208\nprint_info: n_merges         = 0\nprint_info: BOS token        = 2 '<bos>'\nprint_info: EOS token        = 1 '<eos>'\nprint_info: EOT token        = 106 '<end_of_turn>'\nprint_info: UNK token        = 3 '<unk>'\nprint_info: PAD token        = 0 '<pad>'\nprint_info: LF token         = 248 '<0x0A>'\nprint_info: EOG token        = 1 '<eos>'\nprint_info: EOG token        = 106 '<end_of_turn>'\nprint_info: max token length = 48\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_Mapped model buffer size =  2692.37 MiB\n..................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 4096\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = auto\nllama_context: kv_unified    = false\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 0.125\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     1.00 MiB\nllama_kv_cache_iswa: creating non-SWA KV cache, size = 4096 cells\nllama_kv_cache:        CPU KV buffer size =    80.00 MiB\nllama_kv_cache: size =   80.00 MiB (  4096 cells,   5 layers,  1/1 seqs), K (f16):   40.00 MiB, V (f16):   40.00 MiB\nllama_kv_cache_iswa: creating     SWA KV cache, size = 1536 cells\nllama_kv_cache:        CPU KV buffer size =   174.00 MiB\nllama_kv_cache: size =  174.00 MiB (  1536 cells,  29 layers,  1/1 seqs), K (f16):   87.00 MiB, V (f16):   87.00 MiB\nllama_context: Flash Attention was auto, set to enabled\nllama_context:        CPU compute buffer size =   517.12 MiB\nllama_context: graph nodes  = 1369\nllama_context: graph splits = 1\ncommon_init_from_params: added <eos> logit bias = -inf\ncommon_init_from_params: added <end_of_turn> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 12\nmain: chat template is available, enabling conversation muserode (disable it with -n",
        "timestamp": "2025-10-15T07:54:37.139Z"
      }
    ]
  },
  {
    "id": "chat_1760514890204",
    "title": "Hey",
    "timestamp": "2025-10-15T07:54:50.204Z",
    "messages": [
      {
        "id": "1760514719548",
        "role": "user",
        "content": "Hey",
        "timestamp": "2025-10-15T07:51:59.548Z"
      },
      {
        "id": "1760514720678",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-15T07:52:00.678Z"
      },
      {
        "id": "1760514725445",
        "role": "user",
        "content": "hey",
        "timestamp": "2025-10-15T07:52:05.445Z"
      },
      {
        "id": "1760514726011",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-15T07:52:06.011Z"
      },
      {
        "id": "1760514833456",
        "role": "user",
        "content": "hi",
        "timestamp": "2025-10-15T07:53:53.456Z"
      },
      {
        "id": "1760514877139",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 41 key-value pairs and 444 tensors from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Gemma 3 4b It\nllama_model_loader: - kv   3:                           general.finetune str              = it\nllama_model_loader: - kv   4:                           general.basename str              = gemma-3\nllama_model_loader: - kv   5:                         general.size_label str              = 4B\nllama_model_loader: - kv   6:                            general.license str              = gemma\nllama_model_loader: - kv   7:                   general.base_model.count u32              = 1\nllama_model_loader: - kv   8:                  general.base_model.0.name str              = Gemma 3 4b Pt\nllama_model_loader: - kv   9:          general.base_model.0.organization str              = Google\nllama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/google/gemma-3...\nllama_model_loader: - kv  11:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\nllama_model_loader: - kv  12:                      gemma3.context_length u32              = 131072\nllama_model_loader: - kv  13:                    gemma3.embedding_length u32              = 2560\nllama_model_loader: - kv  14:                         gemma3.block_count u32              = 34\nllama_model_loader: - kv  15:                 gemma3.feed_forward_length u32              = 10240\nllama_model_loader: - kv  16:                gemma3.attention.head_count u32              = 8\nllama_model_loader: - kv  17:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  18:                gemma3.attention.key_length u32              = 256\nllama_model_loader: - kv  19:              gemma3.attention.value_length u32              = 256\nllama_model_loader: - kv  20:                      gemma3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  21:            gemma3.attention.sliding_window u32              = 1024\nllama_model_loader: - kv  22:             gemma3.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  23:                   gemma3.rope.scaling.type str              = linear\nllama_model_loader: - kv  24:                 gemma3.rope.scaling.factor f32              = 8.000000\nllama_model_loader: - kv  25:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  26:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  27:                      tokenizer.ggml.tokens arr[str,262208]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  28:                      tokenizer.ggml.scores arr[f32,262208]  = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,262208]  = [3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  31:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  32:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  35:               tokenizer.ggml.add_sep_token bool             = false\nllama_model_loader: - kv  36:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  37:                    tokenizer.chat_template str              = {{ bos_token }}\\n{%- if messages[0]['r...\nllama_model_loader: - kv  38:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - kv  39:               general.quantization_version u32              = 2\nllama_model_loader: - kv  40:                          general.file_type u32              = 17\nllama_model_loader: - type  f32:  205 tensors\nllama_model_loader: - type q5_K:  204 tensors\nllama_model_loader: - type q6_K:   35 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q5_K - Medium\nprint_info: file size   = 2.63 GiB (5.82 BPW) \nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: printing all EOG tokens:\nload:   - 1 ('<eos>')\nload:   - 106 ('<end_of_turn>')\nload: special tokens cache size = 6415\nload: token to piece cache size = 1.9446 MB\nprint_info: arch             = gemma3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 34\nprint_info: n_head           = 8\nprint_info: n_head_kv        = 4\nprint_info: n_rot            = 256\nprint_info: n_swa            = 1024\nprint_info: is_swa_any       = 1\nprint_info: n_embd_head_k    = 256\nprint_info: n_embd_head_v    = 256\nprint_info: n_gqa            = 2\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 6.2e-02\nprint_info: n_ff             = 10240\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 0.125\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 3.88 B\nprint_info: general.name     = Gemma 3 4b It\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 262208\nprint_info: n_merges         = 0\nprint_info: BOS token        = 2 '<bos>'\nprint_info: EOS token        = 1 '<eos>'\nprint_info: EOT token        = 106 '<end_of_turn>'\nprint_info: UNK token        = 3 '<unk>'\nprint_info: PAD token        = 0 '<pad>'\nprint_info: LF token         = 248 '<0x0A>'\nprint_info: EOG token        = 1 '<eos>'\nprint_info: EOG token        = 106 '<end_of_turn>'\nprint_info: max token length = 48\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_Mapped model buffer size =  2692.37 MiB\n..................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 4096\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = auto\nllama_context: kv_unified    = false\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 0.125\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     1.00 MiB\nllama_kv_cache_iswa: creating non-SWA KV cache, size = 4096 cells\nllama_kv_cache:        CPU KV buffer size =    80.00 MiB\nllama_kv_cache: size =   80.00 MiB (  4096 cells,   5 layers,  1/1 seqs), K (f16):   40.00 MiB, V (f16):   40.00 MiB\nllama_kv_cache_iswa: creating     SWA KV cache, size = 1536 cells\nllama_kv_cache:        CPU KV buffer size =   174.00 MiB\nllama_kv_cache: size =  174.00 MiB (  1536 cells,  29 layers,  1/1 seqs), K (f16):   87.00 MiB, V (f16):   87.00 MiB\nllama_context: Flash Attention was auto, set to enabled\nllama_context:        CPU compute buffer size =   517.12 MiB\nllama_context: graph nodes  = 1369\nllama_context: graph splits = 1\ncommon_init_from_params: added <eos> logit bias = -inf\ncommon_init_from_params: added <end_of_turn> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 12\nmain: chat template is available, enabling conversation muserode (disable it with -n",
        "timestamp": "2025-10-15T07:54:37.139Z"
      }
    ]
  },
  {
    "id": "chat_1760514908457",
    "title": "hello",
    "timestamp": "2025-10-15T08:06:22.810Z",
    "messages": [
      {
        "id": "1760514908450",
        "role": "user",
        "content": "hello",
        "timestamp": "2025-10-15T07:55:08.450Z"
      },
      {
        "id": "1760514954513",
        "role": "assistant",
        "content": "Hello there! It\u00e2\u20ac\u2122s great to hear from you. \u00f0\u0178\u02dc\u0160 \n\nWhat\u00e2\u20ac\u2122s on your mind today? Are you looking to learn something new, or do you have a specific question you'd like me to help you with? \n\nI'm here to explain things in a way that makes sense, so don't hesitate to ask anything \u00e2\u20ac\u201c no matter how basic it might seem! \n\nTo help me give you the best explanation, could you tell me:\n\n*   **What topic are you interested in learning about?** (e.g., math, science, history, a specific skill, etc.)\n*   **What\u00e2\u20ac\u2122s your current understanding of the topic?** (e.g., \u00e2\u20ac\u0153I\u00e2\u20ac\u2122ve never heard of this before",
        "timestamp": "2025-10-15T07:55:54.513Z"
      },
      {
        "id": "1760515215990",
        "role": "user",
        "content": "hello",
        "timestamp": "2025-10-15T08:00:15.990Z"
      },
      {
        "id": "1760515262196",
        "role": "assistant",
        "content": "Hello there! It's great to hear from you. \u00f0\u0178\u02dc\u0160 \n\nWhat's on your mind today? Are you looking to learn about something specific, or would you like me to suggest a topic? \n\nI'm here to help in any way I can \u00e2\u20ac\u201c whether it's explaining a tricky concept, working through a problem, or just sparking your curiosity. \n\nTo get us started, perhaps you could tell me:\n\n*   **What subject are you interested in?** (e.g., math, science, history, computer science, literature, etc.)\n*   **Is there a particular topic you'd like to discuss?** (e.g., photosynthesis, the French Revolution, Python programming, Shakespeare, etc.)\n*   **Or, if you\u00e2\u20ac\u2122re not sure, would you like me to suggest something interesting?**",
        "timestamp": "2025-10-15T08:01:02.196Z"
      },
      {
        "id": "1760515564787",
        "role": "user",
        "content": "hey",
        "timestamp": "2025-10-15T08:06:04.787Z"
      },
      {
        "id": "1760515582718",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 41 key-value pairs and 444 tensors from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI_4bit.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = gemma3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Gemma 3 4b It\nllama_model_loader: - kv   3:                           general.finetune str              = it\nllama_model_loader: - kv   4:                           general.basename str              = gemma-3\nllama_model_loader: - kv   5:                         general.size_label str              = 4B\nllama_model_loader: - kv   6:                            general.license str              = gemma\nllama_model_loader: - kv   7:                   general.base_model.count u32              = 1\nllama_model_loader: - kv   8:                  general.base_model.0.name str              = Gemma 3 4b Pt\nllama_model_loader: - kv   9:          general.base_model.0.organization str              = Google\nllama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/google/gemma-3...\nllama_model_loader: - kv  11:                               general.tags arr[str,1]       = [\"image-text-to-text\"]\nllama_model_loader: - kv  12:                      gemma3.context_length u32              = 131072\nllama_model_loader: - kv  13:                    gemma3.embedding_length u32              = 2560\nllama_model_loader: - kv  14:                         gemma3.block_count u32              = 34\nllama_model_loader: - kv  15:                 gemma3.feed_forward_length u32              = 10240\nllama_model_loader: - kv  16:                gemma3.attention.head_count u32              = 8\nllama_model_loader: - kv  17:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  18:                gemma3.attention.key_length u32              = 256\nllama_model_loader: - kv  19:              gemma3.attention.value_length u32              = 256\nllama_model_loader: - kv  20:                      gemma3.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  21:            gemma3.attention.sliding_window u32              = 1024\nllama_model_loader: - kv  22:             gemma3.attention.head_count_kv u32              = 4\nllama_model_loader: - kv  23:                   gemma3.rope.scaling.type str              = linear\nllama_model_loader: - kv  24:                 gemma3.rope.scaling.factor f32              = 8.000000\nllama_model_loader: - kv  25:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  26:                         tokenizer.ggml.pre str              = default\nllama_model_loader: - kv  27:                      tokenizer.ggml.tokens arr[str,262208]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\nllama_model_loader: - kv  28:                      tokenizer.ggml.scores arr[f32,262208]  = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader: - kv  29:                  tokenizer.ggml.token_type arr[i32,262208]  = [3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, ...\nllama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 2\nllama_model_loader: - kv  31:                tokenizer.ggml.eos_token_id u32              = 1\nllama_model_loader: - kv  32:            tokenizer.ggml.unknown_token_id u32              = 3\nllama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  35:               tokenizer.ggml.add_sep_token bool             = false\nllama_model_loader: - kv  36:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  37:                    tokenizer.chat_template str              = {{ bos_token }}\\n{%- if messages[0]['r...\nllama_model_loader: - kv  38:            tokenizer.ggml.add_space_prefix bool             = false\nllama_model_loader: - kv  39:               general.quantization_version u32              = 2\nllama_model_loader: - kv  40:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  205 tensors\nllama_model_loader: - type q4_K:  204 tensors\nllama_model_loader: - type q6_K:   35 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 2.31 GiB (5.12 BPW) \nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: printing all EOG tokens:\nload:   - 1 ('<eos>')\nload:   - 106 ('<end_of_turn>')\nload: special tokens cache size = 6415\nload: token to piece cache size = 1.9446 MB\nprint_info: arch             = gemma3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 34\nprint_info: n_head           = 8\nprint_info: n_head_kv        = 4\nprint_info: n_rot            = 256\nprint_info: n_swa            = 1024\nprint_info: is_swa_any       = 1\nprint_info: n_embd_head_k    = 256\nprint_info: n_embd_head_v    = 256\nprint_info: n_gqa            = 2\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 6.2e-02\nprint_info: n_ff             = 10240\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 0.125\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 3.88 B\nprint_info: general.name     = Gemma 3 4b It\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 262208\nprint_info: n_merges         = 0\nprint_info: BOS token        = 2 '<bos>'\nprint_info: EOS token        = 1 '<eos>'\nprint_info: EOT token        = 106 '<end_of_turn>'\nprint_info: UNK token        = 3 '<unk>'\nprint_info: PAD token        = 0 '<pad>'\nprint_info: LF token         = 248 '<0x0A>'\nprint_info: EOG token        = 1 '<eos>'\nprint_info: EOG token        = 106 '<end_of_turn>'\nprint_info: max token length = 48\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_REPACK model buffer size =  1458.28 MiB\nload_tensors:   CPU_Mapped model buffer size =  2368.31 MiB\n...............................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 4096\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = auto\nllama_context: kv_unified    = false\nllama_context: freq_base     = 1000000.0\nllama_context: freq_scale    = 0.125\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     1.00 MiB\nllama_kv_cache_iswa: creating non-SWA KV cache, size = 4096 cells\nllama_kv_cache:        CPU KV buffer size =    80.00 MiB\nllama_kv_cache: size =   80.00 MiB (  4096 cells,   5 layers,  1/1 seqs), K (f16):   40.00 MiB, V (f16):   40.00 MiB\nllama_kv_cache_iswa: creating     SWA KV cache, size = 1536 cells\nllama_kv_cache:        CPU KV buffer size =   174.00 MiB\nllama_kv_cache: size =  174.00 MiB (  1536 cells,  29 layers,  1/1 seqs), K (f16):   87.00 MiB, V (f16):   87.00 MiB\nllama_context: Flash Attention was auto, set to enabled\nllama_context:        CPU compute buffer size =   517.12 MiB\nllama_context: graph nodes  = 1369\nllama_context: graph splits = 1\ncommon_init_from_params: added <eos> logit bias = -inf\ncommon_init_from_params: added <end_of_turn> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 12\nmain: chat template is available, enabling conversation mode (disable it with -userno-cnv)\n*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?\nmain: chat template example:\n<start_o",
        "timestamp": "2025-10-15T08:06:22.718Z"
      }
    ]
  },
  {
    "id": "chat_1760515598214",
    "title": "hey",
    "timestamp": "2025-10-15T08:06:38.844Z",
    "messages": [
      {
        "id": "1760515598208",
        "role": "user",
        "content": "hey",
        "timestamp": "2025-10-15T08:06:38.208Z"
      },
      {
        "id": "1760515598842",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI_4bit.ggufx'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI_4bit.ggufx\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI_4bit.ggufx', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-15T08:06:38.842Z"
      }
    ]
  },
  {
    "id": "chat_1760518843437",
    "title": "hey",
    "timestamp": "2025-10-15T09:00:44.208Z",
    "messages": [
      {
        "id": "1760518843426",
        "role": "user",
        "content": "hey",
        "timestamp": "2025-10-15T09:00:43.426Z"
      },
      {
        "id": "1760518844203",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI_4bit.ggufx'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI_4bit.ggufx\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\GEMMA_AI_4bit.ggufx', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-15T09:00:44.203Z"
      }
    ]
  },
  {
    "id": "chat_1760518877986",
    "title": "hey",
    "timestamp": "2025-10-15T09:05:20.703Z",
    "messages": [
      {
        "id": "1760518877982",
        "role": "user",
        "content": "hey",
        "timestamp": "2025-10-15T09:01:17.982Z"
      },
      {
        "id": "1760518918951",
        "role": "assistant",
        "content": "\n\nHey there! It\u00e2\u20ac\u2122s great to chat with you. \u00f0\u0178\u02dc\u0160 \n\nWhat\u00e2\u20ac\u2122s on your mind today? Are you wanting to learn something new, or maybe you\u00e2\u20ac\u2122re struggling with a concept and need a little help? \n\nI\u00e2\u20ac\u2122m here to help you understand things better. Don\u00e2\u20ac\u2122t hesitate to ask anything \u00e2\u20ac\u201c no question is too silly! \n\nTo get us started, maybe you could tell me:\n\n*   **What are you interested in learning about today?** (e.g., math, science, history, a specific skill, etc.)\n*   **Or, if you\u00e2\u20ac\u2122re already thinking about a topic, what specifically about it are you finding confusing?**\n\n> lElOaFm ab_yp eursfe_rs\na\n\nm\npIlnetre_rprruipntte:d   b y  suasmeprl\ning time =     120.57 ms /   266 runs   (    0.45 ms per",
        "timestamp": "2025-10-15T09:01:58.951Z"
      },
      {
        "id": "1760518941220",
        "role": "user",
        "content": "how are you",
        "timestamp": "2025-10-15T09:02:21.220Z"
      },
      {
        "id": "1760518971976",
        "role": "assistant",
        "content": "I'm doing great, thanks for asking! I'm always happy to help and assist with any questions or topics you'd like to explore. It's wonderful to have you here!\n\nI've been designed to provide clear and concise explanations, breaking down complex concepts into bite-sized pieces that are easy to understand. My goal is to make learning fun and accessible, so feel free to ask me anything, and I'll do my best to make it interesting and engaging!\n\nWhat would you like to learn about today? Do you have a specific topic in mind, or would you like me to suggest some fun and educational ideas?\n\nl>l aEmOaF_ pbeyr fu_ssearm\np\nl\ne\nrI_nptreirnrtu:p t e d  sbaym pulsienrg\n time",
        "timestamp": "2025-10-15T09:02:51.976Z"
      },
      {
        "id": "1760519085264",
        "role": "user",
        "content": "where in the world is colded",
        "timestamp": "2025-10-15T09:04:45.264Z"
      },
      {
        "id": "1760519120664",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 29 key-value pairs and 398 tensors from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI_qwen.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen\nllama_model_loader: - kv   3:                         general.size_label str              = 4.0B\nllama_model_loader: - kv   4:                            general.license str              = apache-2.0\nllama_model_loader: - kv   5:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-4B-...\nllama_model_loader: - kv   6:                               general.tags arr[str,1]       = [\"text-generation\"]\nllama_model_loader: - kv   7:                          qwen3.block_count u32              = 36\nllama_model_loader: - kv   8:                       qwen3.context_length u32              = 262144\nllama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 2560\nllama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 9728\nllama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 32\nllama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 5000000.000000\nllama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = [\"\u00c4\u00a0 \u00c4\u00a0\", \"\u00c4\u00a0\u00c4\u00a0 \u00c4\u00a0\u00c4\u00a0\", \"i n\", \"\u00c4\u00a0 t\",...\nllama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  26:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  27:               general.quantization_version u32              = 2\nllama_model_loader: - kv  28:                          general.file_type u32              = 17\nllama_model_loader: - type  f32:  145 tensors\nllama_model_loader: - type q5_K:  216 tensors\nllama_model_loader: - type q6_K:   37 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q5_K - Medium\nprint_info: file size   = 2.69 GiB (5.73 BPW) \nload: printing all EOG tokens:\nload:   - 151643 ('<|endoftext|>')\nload:   - 151645 ('<|im_end|>')\nload:   - 151662 ('<|fim_pad|>')\nload:   - 151663 ('<|repo_name|>')\nload:   - 151664 ('<|file_sep|>')\nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 262144\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 36\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 9728\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = -1\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 5000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 262144\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 4.02 B\nprint_info: general.name     = Qwen\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 '\u00c4\u0160'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_Mapped model buffer size =  2749.97 MiB\n..........................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 4096\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = auto\nllama_context: kv_unified    = false\nllama_context: freq_base     = 5000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (262144) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     0.58 MiB\nllama_kv_cache:        CPU KV buffer size =   576.00 MiB\nllama_kv_cache: size =  576.00 MiB (  4096 cells,  36 layers,  1/1 seqs), K (f16):  288.00 MiB, V (f16):  288.00 MiB\nllama_context: Flash Attention was auto, set to enabled\nllama_context:        CPU compute buffer size =   301.75 MiB\nllama_context: graph nodes  = 1267\nllama_context: graph splits = 1\ncommon_init_from_params: added <|endoftext|> logit bias = -inf\ncommon_init_from_params: added <|im_end|> logit bias = -inf\ncommon_init_from_params: added <|fim_pad|> logit bias = -inf\ncommon_init_from_params: added <|repo_name|> logit bias = -inf\ncommon_init_from_params: added <|file_sep|> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 12\nmain: chat template is available, enabuserling conversation mode (disable it with -no-cnv)\n*** User-spe",
        "timestamp": "2025-10-15T09:05:20.664Z"
      }
    ]
  },
  {
    "id": "chat_1760519135274",
    "title": "hey",
    "timestamp": "2025-10-15T09:25:45.652Z",
    "messages": [
      {
        "id": "1760519135265",
        "role": "user",
        "content": "hey",
        "timestamp": "2025-10-15T09:05:35.265Z"
      },
      {
        "id": "1760519149383",
        "role": "assistant",
        "content": "Hey! \u00f0\u0178\u02dc\u0160 How can I help you today? Whether you've got a question, need help understanding a concept, or just want to chat \u00e2\u20ac\u201d I'm here for you! What would you like to explore? \u00f0\u0178\u0161\u20ac",
        "timestamp": "2025-10-15T09:05:49.383Z"
      },
      {
        "id": "1760520324167",
        "role": "user",
        "content": "how are you",
        "timestamp": "2025-10-15T09:25:24.167Z"
      },
      {
        "id": "1760520345612",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 29 key-value pairs and 398 tensors from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen\nllama_model_loader: - kv   3:                         general.size_label str              = 4.0B\nllama_model_loader: - kv   4:                            general.license str              = apache-2.0\nllama_model_loader: - kv   5:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-4B-...\nllama_model_loader: - kv   6:                               general.tags arr[str,1]       = [\"text-generation\"]\nllama_model_loader: - kv   7:                          qwen3.block_count u32              = 36\nllama_model_loader: - kv   8:                       qwen3.context_length u32              = 262144\nllama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 2560\nllama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 9728\nllama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 32\nllama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 5000000.000000\nllama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = [\"\u00c4\u00a0 \u00c4\u00a0\", \"\u00c4\u00a0\u00c4\u00a0 \u00c4\u00a0\u00c4\u00a0\", \"i n\", \"\u00c4\u00a0 t\",...\nllama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  26:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  27:               general.quantization_version u32              = 2\nllama_model_loader: - kv  28:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  145 tensors\nllama_model_loader: - type q4_K:  216 tensors\nllama_model_loader: - type q6_K:   37 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 2.32 GiB (4.95 BPW) \nload: printing all EOG tokens:\nload:   - 151643 ('<|endoftext|>')\nload:   - 151645 ('<|im_end|>')\nload:   - 151662 ('<|fim_pad|>')\nload:   - 151663 ('<|repo_name|>')\nload:   - 151664 ('<|file_sep|>')\nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 262144\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 36\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 9728\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = -1\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 5000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 262144\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 4.02 B\nprint_info: general.name     = Qwen\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 '\u00c4\u0160'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_REPACK model buffer size =  1683.28 MiB\nload_tensors:   CPU_Mapped model buffer size =  2362.55 MiB\n........................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 4096\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = auto\nllama_context: kv_unified    = false\nllama_context: freq_base     = 5000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (262144) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     0.58 MiB\nllama_kv_cache:        CPU KV buffer size =   576.00 MiB\nllama_kv_cache: size =  576.00 MiB (  4096 cells,  36 layers,  1/1 seqs), K (f16):  288.00 MiB, V (f16):  288.00 MiB\nllama_context: Flash Attention was auto, set to enabled\nllama_context:        CPU compute buffer size =   301.75 MiB\nllama_context: graph nodes  = 1267\nllama_context: graph splits = 1\ncommon_init_from_params: added <|endoftext|> logit bias = -inf\ncommon_init_from_params: added <|im_end|> logit bias = -inf\ncommon_init_from_params: added <|fim_pad|> logit bias = -inf\ncommon_init_from_params: added <|repo_name|> logit bias = -inf\ncommon_init_from_params: added <|file_sep|> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 12\nuser",
        "timestamp": "2025-10-15T09:25:45.612Z"
      }
    ]
  },
  {
    "id": "chat_1760556022138",
    "title": "hey",
    "timestamp": "2025-10-15T19:20:22.138Z",
    "messages": [
      {
        "id": "1760519135265",
        "role": "user",
        "content": "hey",
        "timestamp": "2025-10-15T09:05:35.265Z"
      },
      {
        "id": "1760519149383",
        "role": "assistant",
        "content": "Hey! \u00f0\u0178\u02dc\u0160 How can I help you today? Whether you've got a question, need help understanding a concept, or just want to chat \u00e2\u20ac\u201d I'm here for you! What would you like to explore? \u00f0\u0178\u0161\u20ac",
        "timestamp": "2025-10-15T09:05:49.383Z"
      },
      {
        "id": "1760520324167",
        "role": "user",
        "content": "how are you",
        "timestamp": "2025-10-15T09:25:24.167Z"
      },
      {
        "id": "1760520345612",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 29 key-value pairs and 398 tensors from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen\nllama_model_loader: - kv   3:                         general.size_label str              = 4.0B\nllama_model_loader: - kv   4:                            general.license str              = apache-2.0\nllama_model_loader: - kv   5:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-4B-...\nllama_model_loader: - kv   6:                               general.tags arr[str,1]       = [\"text-generation\"]\nllama_model_loader: - kv   7:                          qwen3.block_count u32              = 36\nllama_model_loader: - kv   8:                       qwen3.context_length u32              = 262144\nllama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 2560\nllama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 9728\nllama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 32\nllama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 5000000.000000\nllama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = [\"\u00c4\u00a0 \u00c4\u00a0\", \"\u00c4\u00a0\u00c4\u00a0 \u00c4\u00a0\u00c4\u00a0\", \"i n\", \"\u00c4\u00a0 t\",...\nllama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  26:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  27:               general.quantization_version u32              = 2\nllama_model_loader: - kv  28:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  145 tensors\nllama_model_loader: - type q4_K:  216 tensors\nllama_model_loader: - type q6_K:   37 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 2.32 GiB (4.95 BPW) \nload: printing all EOG tokens:\nload:   - 151643 ('<|endoftext|>')\nload:   - 151645 ('<|im_end|>')\nload:   - 151662 ('<|fim_pad|>')\nload:   - 151663 ('<|repo_name|>')\nload:   - 151664 ('<|file_sep|>')\nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 262144\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 36\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 9728\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = -1\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 5000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 262144\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 4.02 B\nprint_info: general.name     = Qwen\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 '\u00c4\u0160'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_REPACK model buffer size =  1683.28 MiB\nload_tensors:   CPU_Mapped model buffer size =  2362.55 MiB\n........................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 4096\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = auto\nllama_context: kv_unified    = false\nllama_context: freq_base     = 5000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (262144) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     0.58 MiB\nllama_kv_cache:        CPU KV buffer size =   576.00 MiB\nllama_kv_cache: size =  576.00 MiB (  4096 cells,  36 layers,  1/1 seqs), K (f16):  288.00 MiB, V (f16):  288.00 MiB\nllama_context: Flash Attention was auto, set to enabled\nllama_context:        CPU compute buffer size =   301.75 MiB\nllama_context: graph nodes  = 1267\nllama_context: graph splits = 1\ncommon_init_from_params: added <|endoftext|> logit bias = -inf\ncommon_init_from_params: added <|im_end|> logit bias = -inf\ncommon_init_from_params: added <|fim_pad|> logit bias = -inf\ncommon_init_from_params: added <|repo_name|> logit bias = -inf\ncommon_init_from_params: added <|file_sep|> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 12\nuser",
        "timestamp": "2025-10-15T09:25:45.612Z"
      }
    ]
  },
  {
    "id": "chat_1760556022157",
    "title": "hey",
    "timestamp": "2025-10-15T19:20:22.157Z",
    "messages": [
      {
        "id": "1760519135265",
        "role": "user",
        "content": "hey",
        "timestamp": "2025-10-15T09:05:35.265Z"
      },
      {
        "id": "1760519149383",
        "role": "assistant",
        "content": "Hey! \u00f0\u0178\u02dc\u0160 How can I help you today? Whether you've got a question, need help understanding a concept, or just want to chat \u00e2\u20ac\u201d I'm here for you! What would you like to explore? \u00f0\u0178\u0161\u20ac",
        "timestamp": "2025-10-15T09:05:49.383Z"
      },
      {
        "id": "1760520324167",
        "role": "user",
        "content": "how are you",
        "timestamp": "2025-10-15T09:25:24.167Z"
      },
      {
        "id": "1760520345612",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\nllama_model_loader: loaded meta data with 29 key-value pairs and 398 tensors from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Qwen\nllama_model_loader: - kv   3:                         general.size_label str              = 4.0B\nllama_model_loader: - kv   4:                            general.license str              = apache-2.0\nllama_model_loader: - kv   5:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-4B-...\nllama_model_loader: - kv   6:                               general.tags arr[str,1]       = [\"text-generation\"]\nllama_model_loader: - kv   7:                          qwen3.block_count u32              = 36\nllama_model_loader: - kv   8:                       qwen3.context_length u32              = 262144\nllama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 2560\nllama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 9728\nllama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 32\nllama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 5000000.000000\nllama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128\nllama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128\nllama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2\nllama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = [\"\u00c4\u00a0 \u00c4\u00a0\", \"\u00c4\u00a0\u00c4\u00a0 \u00c4\u00a0\u00c4\u00a0\", \"i n\", \"\u00c4\u00a0 t\",...\nllama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645\nllama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151643\nllama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 151643\nllama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = false\nllama_model_loader: - kv  26:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\nllama_model_loader: - kv  27:               general.quantization_version u32              = 2\nllama_model_loader: - kv  28:                          general.file_type u32              = 15\nllama_model_loader: - type  f32:  145 tensors\nllama_model_loader: - type q4_K:  216 tensors\nllama_model_loader: - type q6_K:   37 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 2.32 GiB (4.95 BPW) \nload: printing all EOG tokens:\nload:   - 151643 ('<|endoftext|>')\nload:   - 151645 ('<|im_end|>')\nload:   - 151662 ('<|fim_pad|>')\nload:   - 151663 ('<|repo_name|>')\nload:   - 151664 ('<|file_sep|>')\nload: special tokens cache size = 26\nload: token to piece cache size = 0.9311 MB\nprint_info: arch             = qwen3\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 262144\nprint_info: n_embd           = 2560\nprint_info: n_layer          = 36\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 9728\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = -1\nprint_info: rope type        = 2\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 5000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 262144\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 4B\nprint_info: model params     = 4.02 B\nprint_info: general.name     = Qwen\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 151936\nprint_info: n_merges         = 151387\nprint_info: BOS token        = 151643 '<|endoftext|>'\nprint_info: EOS token        = 151645 '<|im_end|>'\nprint_info: EOT token        = 151645 '<|im_end|>'\nprint_info: PAD token        = 151643 '<|endoftext|>'\nprint_info: LF token         = 198 '\u00c4\u0160'\nprint_info: FIM PRE token    = 151659 '<|fim_prefix|>'\nprint_info: FIM SUF token    = 151661 '<|fim_suffix|>'\nprint_info: FIM MID token    = 151660 '<|fim_middle|>'\nprint_info: FIM PAD token    = 151662 '<|fim_pad|>'\nprint_info: FIM REP token    = 151663 '<|repo_name|>'\nprint_info: FIM SEP token    = 151664 '<|file_sep|>'\nprint_info: EOG token        = 151643 '<|endoftext|>'\nprint_info: EOG token        = 151645 '<|im_end|>'\nprint_info: EOG token        = 151662 '<|fim_pad|>'\nprint_info: EOG token        = 151663 '<|repo_name|>'\nprint_info: EOG token        = 151664 '<|file_sep|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors:   CPU_REPACK model buffer size =  1683.28 MiB\nload_tensors:   CPU_Mapped model buffer size =  2362.55 MiB\n........................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 4096\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 2048\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = auto\nllama_context: kv_unified    = false\nllama_context: freq_base     = 5000000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (262144) -- the full capacity of the model will not be utilized\nllama_context:        CPU  output buffer size =     0.58 MiB\nllama_kv_cache:        CPU KV buffer size =   576.00 MiB\nllama_kv_cache: size =  576.00 MiB (  4096 cells,  36 layers,  1/1 seqs), K (f16):  288.00 MiB, V (f16):  288.00 MiB\nllama_context: Flash Attention was auto, set to enabled\nllama_context:        CPU compute buffer size =   301.75 MiB\nllama_context: graph nodes  = 1267\nllama_context: graph splits = 1\ncommon_init_from_params: added <|endoftext|> logit bias = -inf\ncommon_init_from_params: added <|im_end|> logit bias = -inf\ncommon_init_from_params: added <|fim_pad|> logit bias = -inf\ncommon_init_from_params: added <|repo_name|> logit bias = -inf\ncommon_init_from_params: added <|file_sep|> logit bias = -inf\ncommon_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\ncommon_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\nmain: llama threadpool init, n_threads = 12\nuser",
        "timestamp": "2025-10-15T09:25:45.612Z"
      }
    ]
  },
  {
    "id": "chat_1760874202334",
    "title": "hi",
    "timestamp": "2025-10-19T11:43:33.371Z",
    "messages": [
      {
        "id": "1760874202326",
        "role": "user",
        "content": "hi",
        "timestamp": "2025-10-19T11:43:22.326Z"
      },
      {
        "id": "1760874203780",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-19T11:43:23.780Z"
      },
      {
        "id": "1760874212675",
        "role": "user",
        "content": "hiii",
        "timestamp": "2025-10-19T11:43:32.675Z"
      },
      {
        "id": "1760874213367",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-19T11:43:33.367Z"
      }
    ]
  },
  {
    "id": "chat_1760874361026",
    "title": "hi",
    "timestamp": "2025-10-19T11:46:01.026Z",
    "messages": [
      {
        "id": "1760874202326",
        "role": "user",
        "content": "hi",
        "timestamp": "2025-10-19T11:43:22.326Z"
      },
      {
        "id": "1760874203780",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-19T11:43:23.780Z"
      },
      {
        "id": "1760874212675",
        "role": "user",
        "content": "hiii",
        "timestamp": "2025-10-19T11:43:32.675Z"
      },
      {
        "id": "1760874213367",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-19T11:43:33.367Z"
      }
    ]
  },
  {
    "id": "chat_1760874361028",
    "title": "hi",
    "timestamp": "2025-10-19T11:46:02.486Z",
    "messages": [
      {
        "id": "1760874202326",
        "role": "user",
        "content": "hi",
        "timestamp": "2025-10-19T11:43:22.326Z"
      },
      {
        "id": "1760874203780",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-19T11:43:23.780Z"
      },
      {
        "id": "1760874212675",
        "role": "user",
        "content": "hiii",
        "timestamp": "2025-10-19T11:43:32.675Z"
      },
      {
        "id": "1760874213367",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-19T11:43:33.367Z"
      },
      {
        "id": "1760874362481",
        "role": "user",
        "content": "hi",
        "timestamp": "2025-10-19T11:46:02.481Z"
      }
    ]
  },
  {
    "id": "chat_1760874384361",
    "title": "hi",
    "timestamp": "2025-10-19T11:46:24.361Z",
    "messages": [
      {
        "id": "1760874202326",
        "role": "user",
        "content": "hi",
        "timestamp": "2025-10-19T11:43:22.326Z"
      },
      {
        "id": "1760874203780",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-19T11:43:23.780Z"
      },
      {
        "id": "1760874212675",
        "role": "user",
        "content": "hiii",
        "timestamp": "2025-10-19T11:43:32.675Z"
      },
      {
        "id": "1760874213367",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-19T11:43:33.367Z"
      },
      {
        "id": "1760874362481",
        "role": "user",
        "content": "hi",
        "timestamp": "2025-10-19T11:46:02.481Z"
      }
    ]
  },
  {
    "id": "chat_1760874384364",
    "title": "hi",
    "timestamp": "2025-10-19T11:46:24.364Z",
    "messages": [
      {
        "id": "1760874202326",
        "role": "user",
        "content": "hi",
        "timestamp": "2025-10-19T11:43:22.326Z"
      },
      {
        "id": "1760874203780",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-19T11:43:23.780Z"
      },
      {
        "id": "1760874212675",
        "role": "user",
        "content": "hiii",
        "timestamp": "2025-10-19T11:43:32.675Z"
      },
      {
        "id": "1760874213367",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf'\nllama_model_load: error loading model: llama_model_loader: failed to load model from C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model 'C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-19T11:43:33.367Z"
      },
      {
        "id": "1760874362481",
        "role": "user",
        "content": "hi",
        "timestamp": "2025-10-19T11:46:02.481Z"
      }
    ]
  },
  {
    "id": "chat_1760891505753",
    "title": "explain what is photosynthesis simply",
    "timestamp": "2025-10-19T16:35:08.908Z",
    "messages": [
      {
        "id": "1760891505733",
        "role": "user",
        "content": "explain what is photosynthesis simply",
        "timestamp": "2025-10-19T16:31:45.733Z"
      },
      {
        "id": "1760891513316",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T16:31:53.316Z"
      },
      {
        "id": "1760891522428",
        "role": "user",
        "content": "explain what is photosynthesis simply",
        "timestamp": "2025-10-19T16:32:02.428Z"
      },
      {
        "id": "1760891528502",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T16:32:08.502Z"
      },
      {
        "id": "1760891703907",
        "role": "user",
        "content": "what",
        "timestamp": "2025-10-19T16:35:03.907Z"
      },
      {
        "id": "1760891708904",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T16:35:08.904Z"
      }
    ]
  },
  {
    "id": "chat_1760891583216",
    "title": "whats fire",
    "timestamp": "2025-10-19T16:33:05.974Z",
    "messages": [
      {
        "id": "1760891583211",
        "role": "user",
        "content": "whats fire",
        "timestamp": "2025-10-19T16:33:03.211Z"
      },
      {
        "id": "1760891585971",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T16:33:05.971Z"
      }
    ]
  },
  {
    "id": "chat_1760892641051",
    "title": "what is photosynthersis",
    "timestamp": "2025-10-19T16:53:55.341Z",
    "messages": [
      {
        "id": "1760892641043",
        "role": "user",
        "content": "what is photosynthersis",
        "timestamp": "2025-10-19T16:50:41.043Z"
      },
      {
        "id": "1760892641259",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file '\"C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf\"'\nllama_model_load: error loading model: llama_model_loader: failed to load model from \"C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf\"\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model '\"C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf\"', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-19T16:50:41.259Z"
      },
      {
        "id": "1760892646692",
        "role": "user",
        "content": "hi",
        "timestamp": "2025-10-19T16:50:46.692Z"
      },
      {
        "id": "1760892646759",
        "role": "assistant",
        "content": "build: 6608 (c0bfc57a) with MSVC 19.43.34810.0 for x64\nmain: llama backend init\nmain: load the model and apply lora adapter, if any\ngguf_init_from_file: failed to open GGUF file '\"C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf\"'\nllama_model_load: error loading model: llama_model_loader: failed to load model from \"C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf\"\nllama_model_load_from_file_impl: failed to load model\ncommon_init_from_params: failed to load model '\"C:\\Users\\LG\\Desktop\\Offline AI Standalone\\backend\\PEARL_AI.gguf\"', try reducing --n-gpu-layers if you're running out of VRAM\nmain: error: unable to load model",
        "timestamp": "2025-10-19T16:50:46.759Z"
      },
      {
        "id": "1760892785580",
        "role": "user",
        "content": "hi",
        "timestamp": "2025-10-19T16:53:05.580Z"
      },
      {
        "id": "1760892801725",
        "role": "assistant",
        "content": "system_info: n_threads = 12 (n_threads_batch = 12) / 16 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n\nmain: interactive mode on.\nsampler seed: 38112440\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 2500, n_keep = 1\n\n== Running in interactive mode. ==\n - Press Ctrl+C to interject at any time.\n - Press Return to return control to the AI.\n - To return control without starting a new line, end your input with '/'.\n - If you want to submit another line, end your input with '\\'.\n - Not using system message. To change it, set a different value via -sys PROMPT\n\nHello! I'm thrilled to meet you. I'm here to help you learn and explore new concepts in a fun and easy-to-understand way. What topic would you like to start with today? Are you curious about science, history, language, or something else? Let me know, and I'll do my best to make learning an enjoyable experience for you!",
        "timestamp": "2025-10-19T16:53:21.725Z"
      },
      {
        "id": "1760892809293",
        "role": "user",
        "content": "whats photosynthesis",
        "timestamp": "2025-10-19T16:53:29.293Z"
      },
      {
        "id": "1760892811906",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T16:53:31.906Z"
      },
      {
        "id": "1760892826683",
        "role": "user",
        "content": "whats photosynthesis",
        "timestamp": "2025-10-19T16:53:46.683Z"
      },
      {
        "id": "1760892829059",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T16:53:49.059Z"
      },
      {
        "id": "1760892832357",
        "role": "user",
        "content": "hi",
        "timestamp": "2025-10-19T16:53:52.357Z"
      },
      {
        "id": "1760892835337",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T16:53:55.337Z"
      }
    ]
  },
  {
    "id": "chat_1760893094579",
    "title": "whats fire",
    "timestamp": "2025-10-19T17:13:26.574Z",
    "messages": [
      {
        "id": "1760893094573",
        "role": "user",
        "content": "whats fire",
        "timestamp": "2025-10-19T16:58:14.573Z"
      },
      {
        "id": "1760893227250",
        "role": "user",
        "content": "whats fire\n\n",
        "timestamp": "2025-10-19T17:00:27.250Z"
      },
      {
        "id": "1760893229483",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:00:29.483Z"
      },
      {
        "id": "1760893319885",
        "role": "user",
        "content": "whats fire",
        "timestamp": "2025-10-19T17:01:59.885Z"
      },
      {
        "id": "1760893322160",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:02:02.160Z"
      },
      {
        "id": "1760893757509",
        "role": "user",
        "content": "hello",
        "timestamp": "2025-10-19T17:09:17.509Z"
      },
      {
        "id": "1760893759558",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:09:19.558Z"
      },
      {
        "id": "1760893900719",
        "role": "user",
        "content": "hello",
        "timestamp": "2025-10-19T17:11:40.719Z"
      },
      {
        "id": "1760893911026",
        "role": "assistant",
        "content": "system_info: n_threads = 4 (n_threads_batch = 4) / 16 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n\nmain: interactive mode on.\nsampler seed: 4116111081\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequencx topics into understandable parts, provide examples, and encourage learning. Keep your explanations engaging and educational.user\n\nhelloassistany_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.1tassistant\n\n00, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.600\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 2500, n_keep = 1\n\n== Running in interactive mode. ==\n - Press Ctrl+C to interject at any time.\n - Press Return to return control to the AI.\n - To return control without starting a new line, end your input with '/'.\n - If you want to submit another line, end your input with '\\'.\n - Not using system message. To change it, set a different value via -sys PROMPT\n\nHello! It's great to meet you! I'm here to help you learn and understand new concepts in a fun and easy-to-digest way. Is there a specific topic you'd like to explore or learn more about? Or would you like me to suggest some interesting topics to get us started?\nl\n",
        "timestamp": "2025-10-19T17:11:51.026Z"
      },
      {
        "id": "1760893918444",
        "role": "user",
        "content": "what is photosynthesis",
        "timestamp": "2025-10-19T17:11:58.444Z"
      },
      {
        "id": "1760893920489",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:12:00.489Z"
      },
      {
        "id": "1760893925887",
        "role": "user",
        "content": "hello",
        "timestamp": "2025-10-19T17:12:05.887Z"
      },
      {
        "id": "1760893927895",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:12:07.895Z"
      },
      {
        "id": "1760893980207",
        "role": "user",
        "content": "whats fire",
        "timestamp": "2025-10-19T17:13:00.207Z"
      },
      {
        "id": "1760893988400",
        "role": "assistant",
        "content": "Hi therl.user\n\nwhats fireassistantassistant\n\ne",
        "timestamp": "2025-10-19T17:13:08.400Z"
      },
      {
        "id": "1760893998473",
        "role": "user",
        "content": "whats a cat",
        "timestamp": "2025-10-19T17:13:18.473Z"
      },
      {
        "id": "1760894006561",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:13:26.561Z"
      }
    ]
  },
  {
    "id": "chat_1760894122605",
    "title": "whats fire",
    "timestamp": "2025-10-19T17:15:22.605Z",
    "messages": [
      {
        "id": "1760893094573",
        "role": "user",
        "content": "whats fire",
        "timestamp": "2025-10-19T16:58:14.573Z"
      },
      {
        "id": "1760893227250",
        "role": "user",
        "content": "whats fire\n\n",
        "timestamp": "2025-10-19T17:00:27.250Z"
      },
      {
        "id": "1760893229483",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:00:29.483Z"
      },
      {
        "id": "1760893319885",
        "role": "user",
        "content": "whats fire",
        "timestamp": "2025-10-19T17:01:59.885Z"
      },
      {
        "id": "1760893322160",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:02:02.160Z"
      },
      {
        "id": "1760893757509",
        "role": "user",
        "content": "hello",
        "timestamp": "2025-10-19T17:09:17.509Z"
      },
      {
        "id": "1760893759558",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:09:19.558Z"
      },
      {
        "id": "1760893900719",
        "role": "user",
        "content": "hello",
        "timestamp": "2025-10-19T17:11:40.719Z"
      },
      {
        "id": "1760893911026",
        "role": "assistant",
        "content": "system_info: n_threads = 4 (n_threads_batch = 4) / 16 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n\nmain: interactive mode on.\nsampler seed: 4116111081\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequencx topics into understandable parts, provide examples, and encourage learning. Keep your explanations engaging and educational.user\n\nhelloassistany_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.1tassistant\n\n00, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.600\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 2500, n_keep = 1\n\n== Running in interactive mode. ==\n - Press Ctrl+C to interject at any time.\n - Press Return to return control to the AI.\n - To return control without starting a new line, end your input with '/'.\n - If you want to submit another line, end your input with '\\'.\n - Not using system message. To change it, set a different value via -sys PROMPT\n\nHello! It's great to meet you! I'm here to help you learn and understand new concepts in a fun and easy-to-digest way. Is there a specific topic you'd like to explore or learn more about? Or would you like me to suggest some interesting topics to get us started?\nl\n",
        "timestamp": "2025-10-19T17:11:51.026Z"
      },
      {
        "id": "1760893918444",
        "role": "user",
        "content": "what is photosynthesis",
        "timestamp": "2025-10-19T17:11:58.444Z"
      },
      {
        "id": "1760893920489",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:12:00.489Z"
      },
      {
        "id": "1760893925887",
        "role": "user",
        "content": "hello",
        "timestamp": "2025-10-19T17:12:05.887Z"
      },
      {
        "id": "1760893927895",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:12:07.895Z"
      },
      {
        "id": "1760893980207",
        "role": "user",
        "content": "whats fire",
        "timestamp": "2025-10-19T17:13:00.207Z"
      },
      {
        "id": "1760893988400",
        "role": "assistant",
        "content": "Hi therl.user\n\nwhats fireassistantassistant\n\ne",
        "timestamp": "2025-10-19T17:13:08.400Z"
      },
      {
        "id": "1760893998473",
        "role": "user",
        "content": "whats a cat",
        "timestamp": "2025-10-19T17:13:18.473Z"
      },
      {
        "id": "1760894006561",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:13:26.561Z"
      }
    ]
  },
  {
    "id": "chat_1760894122607",
    "title": "whats fire",
    "timestamp": "2025-10-19T17:15:22.607Z",
    "messages": [
      {
        "id": "1760893094573",
        "role": "user",
        "content": "whats fire",
        "timestamp": "2025-10-19T16:58:14.573Z"
      },
      {
        "id": "1760893227250",
        "role": "user",
        "content": "whats fire\n\n",
        "timestamp": "2025-10-19T17:00:27.250Z"
      },
      {
        "id": "1760893229483",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:00:29.483Z"
      },
      {
        "id": "1760893319885",
        "role": "user",
        "content": "whats fire",
        "timestamp": "2025-10-19T17:01:59.885Z"
      },
      {
        "id": "1760893322160",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:02:02.160Z"
      },
      {
        "id": "1760893757509",
        "role": "user",
        "content": "hello",
        "timestamp": "2025-10-19T17:09:17.509Z"
      },
      {
        "id": "1760893759558",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:09:19.558Z"
      },
      {
        "id": "1760893900719",
        "role": "user",
        "content": "hello",
        "timestamp": "2025-10-19T17:11:40.719Z"
      },
      {
        "id": "1760893911026",
        "role": "assistant",
        "content": "system_info: n_threads = 4 (n_threads_batch = 4) / 16 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n\nmain: interactive mode on.\nsampler seed: 4116111081\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequencx topics into understandable parts, provide examples, and encourage learning. Keep your explanations engaging and educational.user\n\nhelloassistany_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.1tassistant\n\n00, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.600\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 2500, n_keep = 1\n\n== Running in interactive mode. ==\n - Press Ctrl+C to interject at any time.\n - Press Return to return control to the AI.\n - To return control without starting a new line, end your input with '/'.\n - If you want to submit another line, end your input with '\\'.\n - Not using system message. To change it, set a different value via -sys PROMPT\n\nHello! It's great to meet you! I'm here to help you learn and understand new concepts in a fun and easy-to-digest way. Is there a specific topic you'd like to explore or learn more about? Or would you like me to suggest some interesting topics to get us started?\nl\n",
        "timestamp": "2025-10-19T17:11:51.026Z"
      },
      {
        "id": "1760893918444",
        "role": "user",
        "content": "what is photosynthesis",
        "timestamp": "2025-10-19T17:11:58.444Z"
      },
      {
        "id": "1760893920489",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:12:00.489Z"
      },
      {
        "id": "1760893925887",
        "role": "user",
        "content": "hello",
        "timestamp": "2025-10-19T17:12:05.887Z"
      },
      {
        "id": "1760893927895",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:12:07.895Z"
      },
      {
        "id": "1760893980207",
        "role": "user",
        "content": "whats fire",
        "timestamp": "2025-10-19T17:13:00.207Z"
      },
      {
        "id": "1760893988400",
        "role": "assistant",
        "content": "Hi therl.user\n\nwhats fireassistantassistant\n\ne",
        "timestamp": "2025-10-19T17:13:08.400Z"
      },
      {
        "id": "1760893998473",
        "role": "user",
        "content": "whats a cat",
        "timestamp": "2025-10-19T17:13:18.473Z"
      },
      {
        "id": "1760894006561",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:13:26.561Z"
      }
    ]
  },
  {
    "id": "chat_1760894130224",
    "title": "what is fire",
    "timestamp": "2025-10-19T17:18:15.985Z",
    "messages": [
      {
        "id": "1760894130218",
        "role": "user",
        "content": "what is fire",
        "timestamp": "2025-10-19T17:15:30.218Z"
      },
      {
        "id": "1760894266543",
        "role": "assistant",
        "content": "system_info: n_s fireassistantassistant\n\nthreads = 4 (n_threads_batch = 4) / 16 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n\nmain: interactive mode on.\nsampler seed: 2567268028\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.600\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 2500, n_keep = 1\n\n== Running in interactive mode. ==\n - Press Ctrl+C to interject at any time.\n - Press Return to return control to the AI.\n - To return control without starting a new line, end your input with '/'.\n - If you want to submit another line, end your input with '\\'.\n - Not using system message. To change it, set a different value via -sys PROMPT\n\nFire! It's a fundamental force of nature that has fascinated humans for centuries. So, let's dive into the world of fire and explore what it is, how it works, and its importance.\n\n**What is fire?**\n\nFire is a chemical reaction that involves the rapid oxidation of a fuel source, usually in the presence of oxygen. This reaction releases heat, light, and various byproducts, such as carbon dioxide, water vapor, and ash. In simpler terms, fire is a self-sustaining process that involves the combination of fuel, oxygen, and heat to produce a chemical reaction.\n\n**The Fire Triangle**\n\nTo understand how fire works, it's essential to know the three elements that make up the fire triangle:\n\n1. **Fuel**: This is the material that burns, such as wood, gasoline, or propane. The fuel must be combustible, meaning it can react with oxygen to produce heat and light.\n2. **Oxygen**: Fire needs oxygen to keep burning. This is why fires often occur in areas with high oxygen levels, such as in the air or in the presence of combustion fuels.\n3. **Heat**: Fire needs a source of heat to ignite and sustain the combustion process. This heat can come from various sources, such as a spark, flame, or hot surface.\n\n**The Fire Process**\n\nWhen all three elements of the fire triangle are present, a chemical reaction occurs, releasing energy in the form of heat, light, and sound. This process involves several stages:\n\n1. **Ignition**: The fuel is heated to its ignition temperature, causing it to break down and release volatile gases.\n2. **Combustion**: The fuel reacts with oxygen, releasing heat and light energy.\n3. **Sustained combustion**: The heat from the combustion process ignites the fuel, creating a self-sustaining reaction.\n\n**Types of Fire**\n\nThere are several types of fire, including:\n\n1. **Class A fires**: These involve combustible materials, such as paper, wood, and cloth.\n2. **Class B fires**: These involve flammable liquids, such as gasoline, oil, and paint.\n3. **Class C fires**: These involve electrical equipment and appliances.\n4. **Class D fires**: These involve combustible metals, such as magnesium and titanium.\n5. **Class K fires**: These involve cooking oils and greases.\n\n**Importance of Fire**\n\nFire has many uses and benefits, including:\n\n1. **Heating and cooking**: Fire provides warmth and a means of cooking food.\n2. **Lighting**: Fire has been a primary source of light for centuries.\n3. **Industrial processes**: Fire is used in various industrial processes, such as smelting, refining, and manufacturing.\n4. **Firefighting**: Firefighting is a critical service that helps protect people and property from fires.\n\n**Safety and Prevention**\n\nFire safety is crucial, and there are many ways to prevent fires, including:\n\n1. **Using fire extinguishers**: Fire extinguishers are designed to put out small fires before they spread.\n2. **Maintaining equipment**: Regular maintenance of electrical and gas equipment can help prevent fires.\n3. **Preventing arson**: Arson is the intentional act of setting fires, and it's a serious crime.\n\nIn conclusion, fire is a complex and fascinating phenomenon that involves the combination of fuel, oxygen, and heat. Understanding the fire triangle and the different types of fire is essential for safety and prevention. By learning about fire, we can appreciate its importance and take steps to prevent fires from occurring in the first place.\n\nDo you have any questions about fire?\nl\n",
        "timestamp": "2025-10-19T17:17:46.543Z"
      },
      {
        "id": "1760894269626",
        "role": "user",
        "content": "what is water",
        "timestamp": "2025-10-19T17:17:49.626Z"
      },
      {
        "id": "1760894281434",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:18:01.434Z"
      },
      {
        "id": "1760894287851",
        "role": "user",
        "content": "whats food",
        "timestamp": "2025-10-19T17:18:07.851Z"
      },
      {
        "id": "1760894295965",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:18:15.965Z"
      }
    ]
  },
  {
    "id": "chat_1760894504493",
    "title": "whats math in simple terms",
    "timestamp": "2025-10-19T17:38:17.912Z",
    "messages": [
      {
        "id": "1760894504488",
        "role": "user",
        "content": "whats math in simple terms",
        "timestamp": "2025-10-19T17:21:44.488Z"
      },
      {
        "id": "1760894512915",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:21:52.915Z"
      },
      {
        "id": "1760894973316",
        "role": "user",
        "content": "whats food",
        "timestamp": "2025-10-19T17:29:33.316Z"
      },
      {
        "id": "1760894979615",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:29:39.615Z"
      },
      {
        "id": "1760895027597",
        "role": "user",
        "content": "WHATS FIRE",
        "timestamp": "2025-10-19T17:30:27.597Z"
      },
      {
        "id": "1760895035374",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:30:35.374Z"
      },
      {
        "id": "1760895075450",
        "role": "user",
        "content": "where is the tllest building in the world",
        "timestamp": "2025-10-19T17:31:15.450Z"
      },
      {
        "id": "1760895083109",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:31:23.109Z"
      },
      {
        "id": "1760895492862",
        "role": "user",
        "content": "hey",
        "timestamp": "2025-10-19T17:38:12.862Z"
      },
      {
        "id": "1760895497897",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:38:17.897Z"
      }
    ]
  },
  {
    "id": "chat_1760895646832",
    "title": "whats math in simple terms",
    "timestamp": "2025-10-19T17:40:46.832Z",
    "messages": [
      {
        "id": "1760894504488",
        "role": "user",
        "content": "whats math in simple terms",
        "timestamp": "2025-10-19T17:21:44.488Z"
      },
      {
        "id": "1760894512915",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:21:52.915Z"
      },
      {
        "id": "1760894973316",
        "role": "user",
        "content": "whats food",
        "timestamp": "2025-10-19T17:29:33.316Z"
      },
      {
        "id": "1760894979615",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:29:39.615Z"
      },
      {
        "id": "1760895027597",
        "role": "user",
        "content": "WHATS FIRE",
        "timestamp": "2025-10-19T17:30:27.597Z"
      },
      {
        "id": "1760895035374",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:30:35.374Z"
      },
      {
        "id": "1760895075450",
        "role": "user",
        "content": "where is the tllest building in the world",
        "timestamp": "2025-10-19T17:31:15.450Z"
      },
      {
        "id": "1760895083109",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:31:23.109Z"
      },
      {
        "id": "1760895492862",
        "role": "user",
        "content": "hey",
        "timestamp": "2025-10-19T17:38:12.862Z"
      },
      {
        "id": "1760895497897",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:38:17.897Z"
      }
    ]
  },
  {
    "id": "chat_1760895646836",
    "title": "whats math in simple terms",
    "timestamp": "2025-10-19T17:40:55.438Z",
    "messages": [
      {
        "id": "1760894504488",
        "role": "user",
        "content": "whats math in simple terms",
        "timestamp": "2025-10-19T17:21:44.488Z"
      },
      {
        "id": "1760894512915",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:21:52.915Z"
      },
      {
        "id": "1760894973316",
        "role": "user",
        "content": "whats food",
        "timestamp": "2025-10-19T17:29:33.316Z"
      },
      {
        "id": "1760894979615",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:29:39.615Z"
      },
      {
        "id": "1760895027597",
        "role": "user",
        "content": "WHATS FIRE",
        "timestamp": "2025-10-19T17:30:27.597Z"
      },
      {
        "id": "1760895035374",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:30:35.374Z"
      },
      {
        "id": "1760895075450",
        "role": "user",
        "content": "where is the tllest building in the world",
        "timestamp": "2025-10-19T17:31:15.450Z"
      },
      {
        "id": "1760895083109",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:31:23.109Z"
      },
      {
        "id": "1760895492862",
        "role": "user",
        "content": "hey",
        "timestamp": "2025-10-19T17:38:12.862Z"
      },
      {
        "id": "1760895497897",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:38:17.897Z"
      },
      {
        "id": "1760895649874",
        "role": "user",
        "content": "heyyy",
        "timestamp": "2025-10-19T17:40:49.874Z"
      },
      {
        "id": "1760895655418",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:40:55.418Z"
      }
    ]
  },
  {
    "id": "chat_1760895676489",
    "title": "whats cat",
    "timestamp": "2025-10-19T17:50:05.623Z",
    "messages": [
      {
        "id": "1760895676481",
        "role": "user",
        "content": "whats cat",
        "timestamp": "2025-10-19T17:41:16.481Z"
      },
      {
        "id": "1760895684129",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:41:24.129Z"
      },
      {
        "id": "1760895860813",
        "role": "user",
        "content": "whats a dog",
        "timestamp": "2025-10-19T17:44:20.813Z"
      },
      {
        "id": "1760895881760",
        "role": "assistant",
        "content": "\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 16 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACle questions, detailed for complex topics.user\n\nwhats a dogassistantassistant\n\nK = 1 | \n\nmain: interactive mode on.\nsampler seed: 3568163223\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.600\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 2500, n_keep = 1\n\n== Running in interactive mode. ==\n - Press Ctrl+C to interject at any time.\n - Press Return to return control to the AI.\n - To return control without starting a new line, end your input with '/'.\n - If you want to submit another line, end your input with '\\'.\n - Not using system message. To change it, set a different value via -sys PROMPT\n\nA dog is a domesticated mammal that is often kept as a pet. They belong to the family Canidae and are closely related to wolves and jackals. Dogs are known for their loyalty, playfulness, and ability to be trained for various tasks. They come in a wide range of breeds, each with unique characteristics, sizes, and temperaments. Dogs are often used for companionship, hunting, and assistance, and are considered to be one of the most popular pets worldwide.\n\nl>l aEmOaF_ pbeyr fu_ssear\nm\np\nl\neIrn_tperrirnutp:t e d   bsya mupsleirn\ng time =      22.50 ms /   167 runs   (    0.13 ms per token,  7420.90 tokens per second)\n",
        "timestamp": "2025-10-19T17:44:41.760Z"
      },
      {
        "id": "1760895886224",
        "role": "user",
        "content": "what is a dog",
        "timestamp": "2025-10-19T17:44:46.224Z"
      },
      {
        "id": "1760895894298",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:44:54.298Z"
      },
      {
        "id": "1760895916351",
        "role": "user",
        "content": "whats a dog",
        "timestamp": "2025-10-19T17:45:16.351Z"
      },
      {
        "id": "1760895924362",
        "role": "assistant",
        "content": "Hi there",
        "timestamp": "2025-10-19T17:45:24.362Z"
      },
      {
        "id": "1760896126078",
        "role": "user",
        "content": "whats a bus",
        "timestamp": "2025-10-19T17:48:46.078Z"
      },
      {
        "id": "1760896138491",
        "role": "assistant",
        "content": "Hi there<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nHow are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 16 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n\nmain: interactive mode on.\nsampler seed: 3202998470\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.600\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 2500, n_keep = 1\n\n== Running in interactive mode. ==\n - Press Ctrl+C to interject at any time.\n - Press Return to return control to the AI.\n - To return control without starting a new line, end your input with '/'.\n - If you want to submit another line, end your input with '\\'.\n - Not using system message. To change it, set a different value via -sys PROMPT\n\nA bus is a large vehicle designed to transport multiple people, typically along a fixed route. It's a type of public transportation that carries passengers from one place to another, often in a scheduled and organized manner. Buses can be fueled by diesel, gasoline, or electricity, and come in various sizes, from small minibuses to large coaches. They're a popular mode of transportation for daily commutes, school trips, and long-distance travel.\nl\nl>a mEaO_Fp ebryf _ussaemrp\nl\ne\nr\n_Ipnrtienrtr:u p t e ds abmyp luisnegr \ntime =       9.58 ms /   158 runs   (    0.06 ms per token, 16499.58 tokens per second)\n",
        "timestamp": "2025-10-19T17:48:58.491Z"
      },
      {
        "id": "1760896141633",
        "role": "user",
        "content": "whats a car",
        "timestamp": "2025-10-19T17:49:01.633Z"
      },
      {
        "id": "1760896152239",
        "role": "assistant",
        "content": "Hi there<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nHow are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 16 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAlevel to what the user asks - brief for simple questions, detailed for complex topics.user\n\nwhats a carassistantassistant\n\nMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n\nmain: interactive mode on.\nsampler seed: 526813687\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.600\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 2500, n_keep = 1\n\n== Running in interactive mode. ==\n - Press Ctrl+C to interject at any time.\n - Press Return to return control to the AI.\n - To return control without starting a new line, end your input with '/'.\n - If you want to submit another line, end your input with '\\'.\n - Not using system message. To change it, set a different value via -sys PROMPT\n\nA car is a road vehicle, typically with four wheels, powered by an internal combustion engine, electric motor, or other propulsion system. It's designed for transporting people or goods over short to medium distances, usually on paved roads. Cars come in various styles, sizes, and shapes, and are a common mode of transportation worldwide.\n\nl>l aEmOaF_ pbeyr fu_ssearm\np\nl\ne\nrI_nptreirnrtu:p t e d  sbaym pulsienrg\n time =       7.63 ms /   135 runs   (    0.06 ms per token, 17702.60 tokens per second)\n",
        "timestamp": "2025-10-19T17:49:12.239Z"
      },
      {
        "id": "1760896158088",
        "role": "user",
        "content": "whats a donkey",
        "timestamp": "2025-10-19T17:49:18.088Z"
      },
      {
        "id": "1760896173564",
        "role": "assistant",
        "content": "Hi there<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nHow are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 16 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n\nmain: interactive mode on.\nsampler seed: 252072957\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.600\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 2500, n_keep = 1\n\n== Running in interactive mode. ==\n - Press Ctrl+C to interject at any time.\n - Press Return to return control to the AI.\n - To return control without starting a new line, end your input with '/'.\n - If you want to submit another line, end your input with '\\'.\n - Not using system message. To change it, set a different value via -sys PROMPT\n\nA donkey, also known as a burro or ass, is a domesticated mammal that belongs to the equine family. They're closely related to horses and zebras, but are generally smaller and have a more compact build. Donkeys are known for their distinctive loud braying sound, gray or brown coats, and long ears. They're often used as pack animals, especially in rural areas, due to their strength and sure-footedness. Would you like to know more about donkeys?\n\nl>l aEmOaF_ pbeyr fu_ssearm\np\nl\ne\nrI_nptreirnrtu:p t e d  sbaym pulsienrg\n time =      13.29 ms /   172 runs   (    0.08 ms per token, 12939.14 tokens per second)\n",
        "timestamp": "2025-10-19T17:49:33.564Z"
      },
      {
        "id": "1760896205614",
        "role": "assistant",
        "content": "Hi there<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nHow are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n\nsystem_info: n_threads = 4 (n_threads_batch = 4) / 16 | sponses concise but informative. Adapt your detail level to what the user asks - brief for simple questions, detailed for complex CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n\nmain: interactive mode otopics.user\n\nwhats a duckassistantassistant\n\nn.\nsampler seed: 4163257628\nsampler params: \n\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.600\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \ngenerate: n_ctx = 4096, n_batch = 2048, n_predict = 2500, n_keep = 1\n\n== Running in interactive mode. ==\n - Press Ctrl+C to interject at any time.\n - Press Return to return control to the AI.\n - To return control without starting a new line, end your input with '/'.\n - If you want to submit another line, end your input with '\\'.\n - Not using system message. To change it, set a different value via -sys PROMPT\n\nA duck is a type of bird that belongs to the family Anatidae. They are characterized by their broad, flat beaks, webbed feet, and feathers that are often bright and colorful. Ducks are found in many parts of the world, including ponds, lakes, rivers, and wetlands, and are known for their waddling gait and quacking calls. They are omnivores, eating a variety of foods including plants, insects, and small aquatic animals. Some common types of ducks include Mallards, Wood Ducks, and Pekin Ducks.\nl\nl>a mEaO_Fp ebryf _ussaemrp\nl\ne\nr\n_Ipnrtienrtr:up t e d  sbaym pulsienrg\n time =      25.92 ms /   182 runs   (    0.14 ms per token,  7020.79 tokens per second)\n",
        "timestamp": "2025-10-19T17:50:05.614Z"
      }
    ]
  },
  {
    "id": "chat_1760896181858",
    "title": "whats a duck",
    "timestamp": "2025-10-19T17:49:41.858Z",
    "messages": [
      {
        "id": "1760896181852",
        "role": "user",
        "content": "whats a duck",
        "timestamp": "2025-10-19T17:49:41.852Z"
      }
    ]
  }
]